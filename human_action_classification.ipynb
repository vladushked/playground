{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \" + DEVICE)\n",
    "if torch.backends.cudnn.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path_X, file_path_y, seq_length):\n",
    "    X_data = np.array(pd.read_csv(file_path_X, header=None))\n",
    "    y_data = np.array(pd.read_csv(file_path_y, header=None))\n",
    "    \n",
    "    # Shift labels in PyTorch classification models labels start from 0\n",
    "    y_data = y_data - 1\n",
    "    \n",
    "    blocks = X_data.shape[0] / seq_length\n",
    "    \n",
    "    X_seq = np.array(np.split(X_data, blocks, axis=0))\n",
    "    \n",
    "    return X_seq, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"./poses/\"\n",
    "SEQ_LENGTH = 32\n",
    "\n",
    "X_train, y_train = read_data(DATASETS_DIR + \"X_train.txt\",\n",
    "                             DATASETS_DIR + \"Y_train.txt\",\n",
    "                             SEQ_LENGTH)\n",
    "\n",
    "# In repo there aren't labels for validation set, so use test as validation for now\n",
    "X_val, y_val = read_data(DATASETS_DIR + \"X_test.txt\",\n",
    "                         DATASETS_DIR + \"Y_test.txt\",\n",
    "                         SEQ_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                               torch.tensor(y_train, dtype=torch.long).squeeze())\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                             torch.tensor(y_val, dtype=torch.long).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_classes, lstm_hidden_dim=256, fc_hidden_dim=256, n_lstm_layers=2):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        \n",
    "        self._lstm = nn.LSTM(input_size=input_dim,\n",
    "                             hidden_size=lstm_hidden_dim,\n",
    "                             num_layers=n_lstm_layers,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        self._fc = nn.Sequential(nn.Linear(lstm_hidden_dim, fc_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc_hidden_dim, n_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_output, _ = self._lstm.forward(x)\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        fc_output = self._fc.forward(lstm_output)\n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, criterion, batches, phase='train'):\n",
    "    is_train = phase == 'train'\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    n_predictions = 0\n",
    "    \n",
    "    correct_predictions = 0\n",
    "\n",
    "    for X_batch, y_batch in batches:\n",
    "        X_batch = X_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = criterion.forward(y_pred, y_batch)\n",
    "    \n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * y_batch.shape[0]\n",
    "        correct_predictions += (torch.argmax(y_pred, dim=1) == y_batch).sum().item()\n",
    "        n_predictions += y_batch.shape[0]\n",
    "\n",
    "    epoch_loss = epoch_loss / n_predictions\n",
    "    epoch_accuracy = correct_predictions / n_predictions\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, n_epoch, batch_size, train_dataset, val_dataset, backup_name):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    train_batches = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    val_batches = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_accuracy = run_epoch(model, optimizer, criterion, train_batches, phase='train')\n",
    "        val_loss, val_accuracy = run_epoch(model, optimizer, criterion, val_batches, phase='val')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), backup_name)\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch))\n",
    "        print(\"Train loss: \" + str(train_loss) + \", accuracy: \" + str(train_accuracy))\n",
    "        print(\"Val loss: \" + str(val_loss) + \", accuracy: \" + str(val_accuracy) + \"\\n\\n\")\n",
    "        \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 6\n",
    "INPUT_DIM = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "Train loss: 0.8735486226845841, accuracy: 0.6489281767955801\n",
      "Val loss: 0.6211841620039762, accuracy: 0.7440445139975657\n",
      "\n",
      "\n",
      "Epoch: 1\n",
      "Train loss: 0.48130274497343034, accuracy: 0.8121988950276243\n",
      "Val loss: 0.45876633668316125, accuracy: 0.8125543383759346\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Train loss: 0.3702782436299719, accuracy: 0.8585193370165746\n",
      "Val loss: 0.5034662859063835, accuracy: 0.8101199791340636\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Train loss: 0.3491351202047991, accuracy: 0.8681546961325967\n",
      "Val loss: 0.27011767895693073, accuracy: 0.9012345679012346\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Train loss: 0.2769810041340675, accuracy: 0.891889502762431\n",
      "Val loss: 0.313655665158899, accuracy: 0.869414014953921\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "Train loss: 0.20154464583205914, accuracy: 0.9223425414364641\n",
      "Val loss: 0.20805899514211246, accuracy: 0.9231438010780734\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "Train loss: 0.2445332597879415, accuracy: 0.906121546961326\n",
      "Val loss: 0.3991670820106114, accuracy: 0.8497652582159625\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "Train loss: 0.17224384997431086, accuracy: 0.9344530386740332\n",
      "Val loss: 0.32158903264995403, accuracy: 0.892018779342723\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "Train loss: 0.1885981674023096, accuracy: 0.9319779005524862\n",
      "Val loss: 0.24071570668090958, accuracy: 0.9128847157016171\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "Train loss: 0.19384880530241444, accuracy: 0.927646408839779\n",
      "Val loss: 0.2567666179594932, accuracy: 0.9021039819161885\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "Train loss: 0.19337554822639866, accuracy: 0.925878453038674\n",
      "Val loss: 0.3852716677423312, accuracy: 0.8753260302556077\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "Train loss: 0.16536105377239416, accuracy: 0.9390055248618785\n",
      "Val loss: 0.4597050237921575, accuracy: 0.8575899843505478\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "Train loss: 0.1571138994552154, accuracy: 0.9413480662983426\n",
      "Val loss: 0.17627169369967327, accuracy: 0.9285341679707877\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "Train loss: 0.15273290199140158, accuracy: 0.9398453038674033\n",
      "Val loss: 0.23679502981565434, accuracy: 0.9189706138062945\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "Train loss: 0.10815980746436514, accuracy: 0.9611491712707182\n",
      "Val loss: 0.17220754785561754, accuracy: 0.9384454877412624\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "Train loss: 0.12133629483412643, accuracy: 0.9558895027624309\n",
      "Val loss: 0.18002146034804078, accuracy: 0.9398365501651886\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "Train loss: 0.13833611768088946, accuracy: 0.9517348066298342\n",
      "Val loss: 0.22931813937735357, accuracy: 0.9163623717614328\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "Train loss: 0.1293170387481046, accuracy: 0.9534585635359116\n",
      "Val loss: 0.16827500484832936, accuracy: 0.94679186228482\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "Train loss: 0.10733797376491747, accuracy: 0.9620773480662983\n",
      "Val loss: 0.15206738518692042, accuracy: 0.9419231438010781\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "Train loss: 0.10590762594786797, accuracy: 0.9622099447513812\n",
      "Val loss: 0.14282209915193997, accuracy: 0.942792557816032\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "Train loss: 0.0918239034835805, accuracy: 0.9653038674033149\n",
      "Val loss: 0.18685480398813387, accuracy: 0.9297513475917232\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "Train loss: 0.1047938061950121, accuracy: 0.9622541436464088\n",
      "Val loss: 0.11685570925779426, accuracy: 0.952529994783516\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "Train loss: 0.07058675783115197, accuracy: 0.9747624309392265\n",
      "Val loss: 0.20067864179665781, accuracy: 0.9400104329681794\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "Train loss: 0.08766820735364987, accuracy: 0.9688397790055249\n",
      "Val loss: 0.1411727972821085, accuracy: 0.9497478699356634\n",
      "\n",
      "\n",
      "Epoch: 24\n",
      "Train loss: 0.10277271190071632, accuracy: 0.963403314917127\n",
      "Val loss: 0.1530777178196244, accuracy: 0.9521822291775344\n",
      "\n",
      "\n",
      "Epoch: 25\n",
      "Train loss: 0.12322432884653176, accuracy: 0.9549613259668508\n",
      "Val loss: 0.13876111770846125, accuracy: 0.9539210572074421\n",
      "\n",
      "\n",
      "Epoch: 26\n",
      "Train loss: 0.08019538524956828, accuracy: 0.9732154696132597\n",
      "Val loss: 0.14196548254774952, accuracy: 0.9410537297861241\n",
      "\n",
      "\n",
      "Epoch: 27\n",
      "Train loss: 0.08205282129964776, accuracy: 0.9703867403314917\n",
      "Val loss: 0.2708777552003365, accuracy: 0.9031472787341331\n",
      "\n",
      "\n",
      "Epoch: 28\n",
      "Train loss: 0.07426594978620334, accuracy: 0.974232044198895\n",
      "Val loss: 0.11546770097299551, accuracy: 0.9601808381151105\n",
      "\n",
      "\n",
      "Epoch: 29\n",
      "Train loss: 0.0755969595853548, accuracy: 0.9735690607734807\n",
      "Val loss: 0.3046354775860649, accuracy: 0.8941053729786124\n",
      "\n",
      "\n",
      "Epoch: 30\n",
      "Train loss: 0.09760807308867492, accuracy: 0.9656574585635359\n",
      "Val loss: 0.10854558102802168, accuracy: 0.9612241349330551\n",
      "\n",
      "\n",
      "Epoch: 31\n",
      "Train loss: 0.07207354542732404, accuracy: 0.9750276243093923\n",
      "Val loss: 0.07933161259128273, accuracy: 0.9688749782646496\n",
      "\n",
      "\n",
      "Epoch: 32\n",
      "Train loss: 0.0975812500682325, accuracy: 0.964817679558011\n",
      "Val loss: 0.12404691129011139, accuracy: 0.9575725960702487\n",
      "\n",
      "\n",
      "Epoch: 33\n",
      "Train loss: 0.09537964315045605, accuracy: 0.9656132596685083\n",
      "Val loss: 0.38784327351626957, accuracy: 0.8626325856372805\n",
      "\n",
      "\n",
      "Epoch: 34\n",
      "Train loss: 0.09366875433450493, accuracy: 0.9657016574585635\n",
      "Val loss: 0.07989218154476951, accuracy: 0.9680055642496957\n",
      "\n",
      "\n",
      "Epoch: 35\n",
      "Train loss: 0.04696851966758542, accuracy: 0.9833370165745856\n",
      "Val loss: 0.17622217972792195, accuracy: 0.9514866979655712\n",
      "\n",
      "\n",
      "Epoch: 36\n",
      "Train loss: 0.0512240380254569, accuracy: 0.9824530386740331\n",
      "Val loss: 0.09162371938435548, accuracy: 0.9640062597809077\n",
      "\n",
      "\n",
      "Epoch: 37\n",
      "Train loss: 0.05459230239451631, accuracy: 0.9800220994475138\n",
      "Val loss: 0.13405118411540942, accuracy: 0.952529994783516\n",
      "\n",
      "\n",
      "Epoch: 38\n",
      "Train loss: 0.08089525441829507, accuracy: 0.9707845303867403\n",
      "Val loss: 0.10392435214123062, accuracy: 0.9575725960702487\n",
      "\n",
      "\n",
      "Epoch: 39\n",
      "Train loss: 0.06131623744944018, accuracy: 0.9776795580110498\n",
      "Val loss: 0.10115272597895769, accuracy: 0.9641801425838985\n",
      "\n",
      "\n",
      "Epoch: 40\n",
      "Train loss: 0.06199158364649636, accuracy: 0.9775027624309393\n",
      "Val loss: 0.09087322915753134, accuracy: 0.9650495565988524\n",
      "\n",
      "\n",
      "Epoch: 41\n",
      "Train loss: 0.051481724839184166, accuracy: 0.9813480662983426\n",
      "Val loss: 0.1590506696334804, accuracy: 0.9384454877412624\n",
      "\n",
      "\n",
      "Epoch: 42\n",
      "Train loss: 0.06538778840672245, accuracy: 0.9775911602209945\n",
      "Val loss: 0.22317246896386603, accuracy: 0.9297513475917232\n",
      "\n",
      "\n",
      "Epoch: 43\n",
      "Train loss: 0.0629294374865063, accuracy: 0.9772375690607735\n",
      "Val loss: 0.08073770713501655, accuracy: 0.9683533298556772\n",
      "\n",
      "\n",
      "Epoch: 44\n",
      "Train loss: 0.04425359480803349, accuracy: 0.9844861878453038\n",
      "Val loss: 0.058100116569523644, accuracy: 0.9746131107633456\n",
      "\n",
      "\n",
      "Epoch: 45\n",
      "Train loss: 0.05855927577252546, accuracy: 0.9794475138121547\n",
      "Val loss: 0.14888879645115743, accuracy: 0.9436619718309859\n",
      "\n",
      "\n",
      "Epoch: 46\n",
      "Train loss: 0.08699687327160525, accuracy: 0.9695469613259668\n",
      "Val loss: 0.13044291378562928, accuracy: 0.9544427056164145\n",
      "\n",
      "\n",
      "Epoch: 47\n",
      "Train loss: 0.07487187519443134, accuracy: 0.9748066298342541\n",
      "Val loss: 0.07736094586014841, accuracy: 0.9688749782646496\n",
      "\n",
      "\n",
      "Epoch: 48\n",
      "Train loss: 0.05951084620550851, accuracy: 0.9797569060773481\n",
      "Val loss: 0.11220023536174023, accuracy: 0.9613980177360459\n",
      "\n",
      "\n",
      "Epoch: 49\n",
      "Train loss: 0.05211047360593793, accuracy: 0.981524861878453\n",
      "Val loss: 0.13659915878864518, accuracy: 0.9547904712223961\n",
      "\n",
      "\n",
      "Epoch: 50\n",
      "Train loss: 0.0654746382687632, accuracy: 0.9777237569060774\n",
      "Val loss: 0.07129768802977254, accuracy: 0.9721787515214745\n",
      "\n",
      "\n",
      "Epoch: 51\n",
      "Train loss: 0.07681369263658207, accuracy: 0.9734806629834254\n",
      "Val loss: 0.20124599370539134, accuracy: 0.9337506520605112\n",
      "\n",
      "\n",
      "Epoch: 52\n",
      "Train loss: 0.044812741151172154, accuracy: 0.9838232044198895\n",
      "Val loss: 0.07354985906765003, accuracy: 0.9754825247782994\n",
      "\n",
      "\n",
      "Epoch: 53\n",
      "Train loss: 0.04169330514596971, accuracy: 0.9848397790055249\n",
      "Val loss: 0.08835384215030306, accuracy: 0.968527212658668\n",
      "\n",
      "\n",
      "Epoch: 54\n",
      "Train loss: 0.056397179180745924, accuracy: 0.980243093922652\n",
      "Val loss: 0.21074753585173564, accuracy: 0.9198400278212485\n",
      "\n",
      "\n",
      "Epoch: 55\n",
      "Train loss: 0.06540772032921424, accuracy: 0.9768839779005525\n",
      "Val loss: 0.09616019399096185, accuracy: 0.9629629629629629\n",
      "\n",
      "\n",
      "Epoch: 56\n",
      "Train loss: 0.07633163423087057, accuracy: 0.9737016574585635\n",
      "Val loss: 0.11539241931760864, accuracy: 0.9573987132672579\n",
      "\n",
      "\n",
      "Epoch: 57\n",
      "Train loss: 0.055633779746886776, accuracy: 0.9805966850828729\n",
      "Val loss: 0.120142063001071, accuracy: 0.9657450878108155\n",
      "\n",
      "\n",
      "Epoch: 58\n",
      "Train loss: 0.06838037114775641, accuracy: 0.9762651933701657\n",
      "Val loss: 0.0775641988276421, accuracy: 0.97339593114241\n",
      "\n",
      "\n",
      "Epoch: 59\n",
      "Train loss: 0.053590912112544256, accuracy: 0.9809060773480663\n",
      "Val loss: 0.09695740938079764, accuracy: 0.96452790818988\n",
      "\n",
      "\n",
      "Epoch: 60\n",
      "Train loss: 0.0628714390222539, accuracy: 0.978342541436464\n",
      "Val loss: 0.09058009887954965, accuracy: 0.9657450878108155\n",
      "\n",
      "\n",
      "Epoch: 61\n",
      "Train loss: 0.045393846311621903, accuracy: 0.9840441988950276\n",
      "Val loss: 0.06536958760441548, accuracy: 0.9735698139454008\n",
      "\n",
      "\n",
      "Epoch: 62\n",
      "Train loss: 0.05751636609517408, accuracy: 0.97953591160221\n",
      "Val loss: 0.14220434166765117, accuracy: 0.9497478699356634\n",
      "\n",
      "\n",
      "Epoch: 63\n",
      "Train loss: 0.08074402875827821, accuracy: 0.9716685082872928\n",
      "Val loss: 0.07930320302022702, accuracy: 0.9697443922796035\n",
      "\n",
      "\n",
      "Epoch: 64\n",
      "Train loss: 0.04321444615804029, accuracy: 0.9851491712707182\n",
      "Val loss: 0.08943844097148232, accuracy: 0.9680055642496957\n",
      "\n",
      "\n",
      "Epoch: 65\n",
      "Train loss: 0.05079126861872594, accuracy: 0.9826298342541436\n",
      "Val loss: 0.06493218757685769, accuracy: 0.9721787515214745\n",
      "\n",
      "\n",
      "Epoch: 66\n",
      "Train loss: 0.050103869856186006, accuracy: 0.9824972375690608\n",
      "Val loss: 0.08265407143692353, accuracy: 0.970961571900539\n",
      "\n",
      "\n",
      "Epoch: 67\n",
      "Train loss: 0.03507644276016325, accuracy: 0.9877127071823204\n",
      "Val loss: 0.0510503365727251, accuracy: 0.9780907668231612\n",
      "\n",
      "\n",
      "Epoch: 68\n",
      "Train loss: 0.03688339269977289, accuracy: 0.9868729281767956\n",
      "Val loss: 0.06409464690922057, accuracy: 0.974960876369327\n",
      "\n",
      "\n",
      "Epoch: 69\n",
      "Train loss: 0.041359042481246576, accuracy: 0.9853259668508287\n",
      "Val loss: 0.0902351198168243, accuracy: 0.9699182750825943\n",
      "\n",
      "\n",
      "Epoch: 70\n",
      "Train loss: 0.03544109247172077, accuracy: 0.9874475138121547\n",
      "Val loss: 0.06698930089201968, accuracy: 0.9723526343244653\n",
      "\n",
      "\n",
      "Epoch: 71\n",
      "Train loss: 0.03485896200435596, accuracy: 0.9878453038674033\n",
      "Val loss: 0.07941066838737426, accuracy: 0.9711354547035298\n",
      "\n",
      "\n",
      "Epoch: 72\n",
      "Train loss: 0.04210835051470699, accuracy: 0.9854143646408839\n",
      "Val loss: 0.27309605946031246, accuracy: 0.9167101373674144\n",
      "\n",
      "\n",
      "Epoch: 73\n",
      "Train loss: 0.07559601816384749, accuracy: 0.9754696132596685\n",
      "Val loss: 0.10141229745728485, accuracy: 0.9673100330377326\n",
      "\n",
      "\n",
      "Epoch: 74\n",
      "Train loss: 0.04357416177816813, accuracy: 0.9849723756906077\n",
      "Val loss: 0.08438794725363463, accuracy: 0.9711354547035298\n",
      "\n",
      "\n",
      "Epoch: 75\n",
      "Train loss: 0.030860471100808837, accuracy: 0.9887734806629834\n",
      "Val loss: 0.06564738088682306, accuracy: 0.9798295948530691\n",
      "\n",
      "\n",
      "Epoch: 76\n",
      "Train loss: 0.036541297235294604, accuracy: 0.9881546961325967\n",
      "Val loss: 0.07934415308124455, accuracy: 0.9725265171274561\n",
      "\n",
      "\n",
      "Epoch: 77\n",
      "Train loss: 0.03869030817890365, accuracy: 0.9861215469613259\n",
      "Val loss: 0.11631594676814944, accuracy: 0.9643540253868892\n",
      "\n",
      "\n",
      "Epoch: 78\n",
      "Train loss: 0.036192047033067894, accuracy: 0.9877569060773481\n",
      "Val loss: 0.11903673065906457, accuracy: 0.9551382368283776\n",
      "\n",
      "\n",
      "Epoch: 79\n",
      "Train loss: 0.05795359878571838, accuracy: 0.9792707182320441\n",
      "Val loss: 0.09058221255546416, accuracy: 0.9697443922796035\n",
      "\n",
      "\n",
      "Epoch: 80\n",
      "Train loss: 0.040659130261747875, accuracy: 0.9848397790055249\n",
      "Val loss: 0.12191057274405673, accuracy: 0.9587897756911842\n",
      "\n",
      "\n",
      "Epoch: 81\n",
      "Train loss: 0.04481880535572273, accuracy: 0.984707182320442\n",
      "Val loss: 0.1233115003656721, accuracy: 0.9591375412971657\n",
      "\n",
      "\n",
      "Epoch: 82\n",
      "Train loss: 0.06331975865306445, accuracy: 0.9776795580110498\n",
      "Val loss: 0.10492566676817347, accuracy: 0.970961571900539\n",
      "\n",
      "\n",
      "Epoch: 83\n",
      "Train loss: 0.04256203787514518, accuracy: 0.984707182320442\n",
      "Val loss: 0.11040246008239304, accuracy: 0.9648756737958616\n",
      "\n",
      "\n",
      "Epoch: 84\n",
      "Train loss: 0.03762714751888046, accuracy: 0.9879779005524861\n",
      "Val loss: 0.0784957536592911, accuracy: 0.9720048687184838\n",
      "\n",
      "\n",
      "Epoch: 85\n",
      "Train loss: 0.0230426168100116, accuracy: 0.9927513812154696\n",
      "Val loss: 0.05796452499581156, accuracy: 0.9810467744740046\n",
      "\n",
      "\n",
      "Epoch: 86\n",
      "Train loss: 0.03029545205979716, accuracy: 0.9897458563535911\n",
      "Val loss: 0.06605394684588234, accuracy: 0.975830290384281\n",
      "\n",
      "\n",
      "Epoch: 87\n",
      "Train loss: 0.04663788862835142, accuracy: 0.9848839779005525\n",
      "Val loss: 0.08701971364890956, accuracy: 0.9728742827334377\n",
      "\n",
      "\n",
      "Epoch: 88\n",
      "Train loss: 0.04618885129772497, accuracy: 0.9849281767955801\n",
      "Val loss: 0.23499713032428876, accuracy: 0.9300991131977048\n",
      "\n",
      "\n",
      "Epoch: 89\n",
      "Train loss: 0.06090077068450866, accuracy: 0.9796685082872928\n",
      "Val loss: 0.14679802382028134, accuracy: 0.9553121196313684\n",
      "\n",
      "\n",
      "Epoch: 90\n",
      "Train loss: 0.05378697729046914, accuracy: 0.9809502762430939\n",
      "Val loss: 0.0931847726414798, accuracy: 0.9706138062945575\n",
      "\n",
      "\n",
      "Epoch: 91\n",
      "Train loss: 0.05114451131438682, accuracy: 0.9824530386740331\n",
      "Val loss: 0.1281534519503502, accuracy: 0.9601808381151105\n",
      "\n",
      "\n",
      "Epoch: 92\n",
      "Train loss: 0.0467184322129791, accuracy: 0.9832044198895028\n",
      "Val loss: 0.09721326736259316, accuracy: 0.9674839158407234\n",
      "\n",
      "\n",
      "Epoch: 93\n",
      "Train loss: 0.05389563671920387, accuracy: 0.9803314917127072\n",
      "Val loss: 0.0960857172550666, accuracy: 0.9673100330377326\n",
      "\n",
      "\n",
      "Epoch: 94\n",
      "Train loss: 0.048165335642040105, accuracy: 0.9834254143646409\n",
      "Val loss: 0.1704642465234064, accuracy: 0.9394887845592071\n",
      "\n",
      "\n",
      "Epoch: 95\n",
      "Train loss: 0.04513653215453111, accuracy: 0.9847513812154696\n",
      "Val loss: 0.11685481813445657, accuracy: 0.9636584941749261\n",
      "\n",
      "\n",
      "Epoch: 96\n",
      "Train loss: 0.037053695992046956, accuracy: 0.9872265193370166\n",
      "Val loss: 0.14966437045596195, accuracy: 0.9558337680403408\n",
      "\n",
      "\n",
      "Epoch: 97\n",
      "Train loss: 0.07355283905870348, accuracy: 0.9749834254143647\n",
      "Val loss: 0.22490323309782892, accuracy: 0.9257520431229351\n",
      "\n",
      "\n",
      "Epoch: 98\n",
      "Train loss: 0.05021775581395428, accuracy: 0.9823204419889503\n",
      "Val loss: 0.09092609374476246, accuracy: 0.9659189706138063\n",
      "\n",
      "\n",
      "Epoch: 99\n",
      "Train loss: 0.05387003534121017, accuracy: 0.982718232044199\n",
      "Val loss: 0.08504296933522287, accuracy: 0.970961571900539\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0.8735486226845841,\n",
       "  0.48130274497343034,\n",
       "  0.3702782436299719,\n",
       "  0.3491351202047991,\n",
       "  0.2769810041340675,\n",
       "  0.20154464583205914,\n",
       "  0.2445332597879415,\n",
       "  0.17224384997431086,\n",
       "  0.1885981674023096,\n",
       "  0.19384880530241444,\n",
       "  0.19337554822639866,\n",
       "  0.16536105377239416,\n",
       "  0.1571138994552154,\n",
       "  0.15273290199140158,\n",
       "  0.10815980746436514,\n",
       "  0.12133629483412643,\n",
       "  0.13833611768088946,\n",
       "  0.1293170387481046,\n",
       "  0.10733797376491747,\n",
       "  0.10590762594786797,\n",
       "  0.0918239034835805,\n",
       "  0.1047938061950121,\n",
       "  0.07058675783115197,\n",
       "  0.08766820735364987,\n",
       "  0.10277271190071632,\n",
       "  0.12322432884653176,\n",
       "  0.08019538524956828,\n",
       "  0.08205282129964776,\n",
       "  0.07426594978620334,\n",
       "  0.0755969595853548,\n",
       "  0.09760807308867492,\n",
       "  0.07207354542732404,\n",
       "  0.0975812500682325,\n",
       "  0.09537964315045605,\n",
       "  0.09366875433450493,\n",
       "  0.04696851966758542,\n",
       "  0.0512240380254569,\n",
       "  0.05459230239451631,\n",
       "  0.08089525441829507,\n",
       "  0.06131623744944018,\n",
       "  0.06199158364649636,\n",
       "  0.051481724839184166,\n",
       "  0.06538778840672245,\n",
       "  0.0629294374865063,\n",
       "  0.04425359480803349,\n",
       "  0.05855927577252546,\n",
       "  0.08699687327160525,\n",
       "  0.07487187519443134,\n",
       "  0.05951084620550851,\n",
       "  0.05211047360593793,\n",
       "  0.0654746382687632,\n",
       "  0.07681369263658207,\n",
       "  0.044812741151172154,\n",
       "  0.04169330514596971,\n",
       "  0.056397179180745924,\n",
       "  0.06540772032921424,\n",
       "  0.07633163423087057,\n",
       "  0.055633779746886776,\n",
       "  0.06838037114775641,\n",
       "  0.053590912112544256,\n",
       "  0.0628714390222539,\n",
       "  0.045393846311621903,\n",
       "  0.05751636609517408,\n",
       "  0.08074402875827821,\n",
       "  0.04321444615804029,\n",
       "  0.05079126861872594,\n",
       "  0.050103869856186006,\n",
       "  0.03507644276016325,\n",
       "  0.03688339269977289,\n",
       "  0.041359042481246576,\n",
       "  0.03544109247172077,\n",
       "  0.03485896200435596,\n",
       "  0.04210835051470699,\n",
       "  0.07559601816384749,\n",
       "  0.04357416177816813,\n",
       "  0.030860471100808837,\n",
       "  0.036541297235294604,\n",
       "  0.03869030817890365,\n",
       "  0.036192047033067894,\n",
       "  0.05795359878571838,\n",
       "  0.040659130261747875,\n",
       "  0.04481880535572273,\n",
       "  0.06331975865306445,\n",
       "  0.04256203787514518,\n",
       "  0.03762714751888046,\n",
       "  0.0230426168100116,\n",
       "  0.03029545205979716,\n",
       "  0.04663788862835142,\n",
       "  0.04618885129772497,\n",
       "  0.06090077068450866,\n",
       "  0.05378697729046914,\n",
       "  0.05114451131438682,\n",
       "  0.0467184322129791,\n",
       "  0.05389563671920387,\n",
       "  0.048165335642040105,\n",
       "  0.04513653215453111,\n",
       "  0.037053695992046956,\n",
       "  0.07355283905870348,\n",
       "  0.05021775581395428,\n",
       "  0.05387003534121017],\n",
       " [0.6489281767955801,\n",
       "  0.8121988950276243,\n",
       "  0.8585193370165746,\n",
       "  0.8681546961325967,\n",
       "  0.891889502762431,\n",
       "  0.9223425414364641,\n",
       "  0.906121546961326,\n",
       "  0.9344530386740332,\n",
       "  0.9319779005524862,\n",
       "  0.927646408839779,\n",
       "  0.925878453038674,\n",
       "  0.9390055248618785,\n",
       "  0.9413480662983426,\n",
       "  0.9398453038674033,\n",
       "  0.9611491712707182,\n",
       "  0.9558895027624309,\n",
       "  0.9517348066298342,\n",
       "  0.9534585635359116,\n",
       "  0.9620773480662983,\n",
       "  0.9622099447513812,\n",
       "  0.9653038674033149,\n",
       "  0.9622541436464088,\n",
       "  0.9747624309392265,\n",
       "  0.9688397790055249,\n",
       "  0.963403314917127,\n",
       "  0.9549613259668508,\n",
       "  0.9732154696132597,\n",
       "  0.9703867403314917,\n",
       "  0.974232044198895,\n",
       "  0.9735690607734807,\n",
       "  0.9656574585635359,\n",
       "  0.9750276243093923,\n",
       "  0.964817679558011,\n",
       "  0.9656132596685083,\n",
       "  0.9657016574585635,\n",
       "  0.9833370165745856,\n",
       "  0.9824530386740331,\n",
       "  0.9800220994475138,\n",
       "  0.9707845303867403,\n",
       "  0.9776795580110498,\n",
       "  0.9775027624309393,\n",
       "  0.9813480662983426,\n",
       "  0.9775911602209945,\n",
       "  0.9772375690607735,\n",
       "  0.9844861878453038,\n",
       "  0.9794475138121547,\n",
       "  0.9695469613259668,\n",
       "  0.9748066298342541,\n",
       "  0.9797569060773481,\n",
       "  0.981524861878453,\n",
       "  0.9777237569060774,\n",
       "  0.9734806629834254,\n",
       "  0.9838232044198895,\n",
       "  0.9848397790055249,\n",
       "  0.980243093922652,\n",
       "  0.9768839779005525,\n",
       "  0.9737016574585635,\n",
       "  0.9805966850828729,\n",
       "  0.9762651933701657,\n",
       "  0.9809060773480663,\n",
       "  0.978342541436464,\n",
       "  0.9840441988950276,\n",
       "  0.97953591160221,\n",
       "  0.9716685082872928,\n",
       "  0.9851491712707182,\n",
       "  0.9826298342541436,\n",
       "  0.9824972375690608,\n",
       "  0.9877127071823204,\n",
       "  0.9868729281767956,\n",
       "  0.9853259668508287,\n",
       "  0.9874475138121547,\n",
       "  0.9878453038674033,\n",
       "  0.9854143646408839,\n",
       "  0.9754696132596685,\n",
       "  0.9849723756906077,\n",
       "  0.9887734806629834,\n",
       "  0.9881546961325967,\n",
       "  0.9861215469613259,\n",
       "  0.9877569060773481,\n",
       "  0.9792707182320441,\n",
       "  0.9848397790055249,\n",
       "  0.984707182320442,\n",
       "  0.9776795580110498,\n",
       "  0.984707182320442,\n",
       "  0.9879779005524861,\n",
       "  0.9927513812154696,\n",
       "  0.9897458563535911,\n",
       "  0.9848839779005525,\n",
       "  0.9849281767955801,\n",
       "  0.9796685082872928,\n",
       "  0.9809502762430939,\n",
       "  0.9824530386740331,\n",
       "  0.9832044198895028,\n",
       "  0.9803314917127072,\n",
       "  0.9834254143646409,\n",
       "  0.9847513812154696,\n",
       "  0.9872265193370166,\n",
       "  0.9749834254143647,\n",
       "  0.9823204419889503,\n",
       "  0.982718232044199],\n",
       " [0.6211841620039762,\n",
       "  0.45876633668316125,\n",
       "  0.5034662859063835,\n",
       "  0.27011767895693073,\n",
       "  0.313655665158899,\n",
       "  0.20805899514211246,\n",
       "  0.3991670820106114,\n",
       "  0.32158903264995403,\n",
       "  0.24071570668090958,\n",
       "  0.2567666179594932,\n",
       "  0.3852716677423312,\n",
       "  0.4597050237921575,\n",
       "  0.17627169369967327,\n",
       "  0.23679502981565434,\n",
       "  0.17220754785561754,\n",
       "  0.18002146034804078,\n",
       "  0.22931813937735357,\n",
       "  0.16827500484832936,\n",
       "  0.15206738518692042,\n",
       "  0.14282209915193997,\n",
       "  0.18685480398813387,\n",
       "  0.11685570925779426,\n",
       "  0.20067864179665781,\n",
       "  0.1411727972821085,\n",
       "  0.1530777178196244,\n",
       "  0.13876111770846125,\n",
       "  0.14196548254774952,\n",
       "  0.2708777552003365,\n",
       "  0.11546770097299551,\n",
       "  0.3046354775860649,\n",
       "  0.10854558102802168,\n",
       "  0.07933161259128273,\n",
       "  0.12404691129011139,\n",
       "  0.38784327351626957,\n",
       "  0.07989218154476951,\n",
       "  0.17622217972792195,\n",
       "  0.09162371938435548,\n",
       "  0.13405118411540942,\n",
       "  0.10392435214123062,\n",
       "  0.10115272597895769,\n",
       "  0.09087322915753134,\n",
       "  0.1590506696334804,\n",
       "  0.22317246896386603,\n",
       "  0.08073770713501655,\n",
       "  0.058100116569523644,\n",
       "  0.14888879645115743,\n",
       "  0.13044291378562928,\n",
       "  0.07736094586014841,\n",
       "  0.11220023536174023,\n",
       "  0.13659915878864518,\n",
       "  0.07129768802977254,\n",
       "  0.20124599370539134,\n",
       "  0.07354985906765003,\n",
       "  0.08835384215030306,\n",
       "  0.21074753585173564,\n",
       "  0.09616019399096185,\n",
       "  0.11539241931760864,\n",
       "  0.120142063001071,\n",
       "  0.0775641988276421,\n",
       "  0.09695740938079764,\n",
       "  0.09058009887954965,\n",
       "  0.06536958760441548,\n",
       "  0.14220434166765117,\n",
       "  0.07930320302022702,\n",
       "  0.08943844097148232,\n",
       "  0.06493218757685769,\n",
       "  0.08265407143692353,\n",
       "  0.0510503365727251,\n",
       "  0.06409464690922057,\n",
       "  0.0902351198168243,\n",
       "  0.06698930089201968,\n",
       "  0.07941066838737426,\n",
       "  0.27309605946031246,\n",
       "  0.10141229745728485,\n",
       "  0.08438794725363463,\n",
       "  0.06564738088682306,\n",
       "  0.07934415308124455,\n",
       "  0.11631594676814944,\n",
       "  0.11903673065906457,\n",
       "  0.09058221255546416,\n",
       "  0.12191057274405673,\n",
       "  0.1233115003656721,\n",
       "  0.10492566676817347,\n",
       "  0.11040246008239304,\n",
       "  0.0784957536592911,\n",
       "  0.05796452499581156,\n",
       "  0.06605394684588234,\n",
       "  0.08701971364890956,\n",
       "  0.23499713032428876,\n",
       "  0.14679802382028134,\n",
       "  0.0931847726414798,\n",
       "  0.1281534519503502,\n",
       "  0.09721326736259316,\n",
       "  0.0960857172550666,\n",
       "  0.1704642465234064,\n",
       "  0.11685481813445657,\n",
       "  0.14966437045596195,\n",
       "  0.22490323309782892,\n",
       "  0.09092609374476246,\n",
       "  0.08504296933522287],\n",
       " [0.7440445139975657,\n",
       "  0.8125543383759346,\n",
       "  0.8101199791340636,\n",
       "  0.9012345679012346,\n",
       "  0.869414014953921,\n",
       "  0.9231438010780734,\n",
       "  0.8497652582159625,\n",
       "  0.892018779342723,\n",
       "  0.9128847157016171,\n",
       "  0.9021039819161885,\n",
       "  0.8753260302556077,\n",
       "  0.8575899843505478,\n",
       "  0.9285341679707877,\n",
       "  0.9189706138062945,\n",
       "  0.9384454877412624,\n",
       "  0.9398365501651886,\n",
       "  0.9163623717614328,\n",
       "  0.94679186228482,\n",
       "  0.9419231438010781,\n",
       "  0.942792557816032,\n",
       "  0.9297513475917232,\n",
       "  0.952529994783516,\n",
       "  0.9400104329681794,\n",
       "  0.9497478699356634,\n",
       "  0.9521822291775344,\n",
       "  0.9539210572074421,\n",
       "  0.9410537297861241,\n",
       "  0.9031472787341331,\n",
       "  0.9601808381151105,\n",
       "  0.8941053729786124,\n",
       "  0.9612241349330551,\n",
       "  0.9688749782646496,\n",
       "  0.9575725960702487,\n",
       "  0.8626325856372805,\n",
       "  0.9680055642496957,\n",
       "  0.9514866979655712,\n",
       "  0.9640062597809077,\n",
       "  0.952529994783516,\n",
       "  0.9575725960702487,\n",
       "  0.9641801425838985,\n",
       "  0.9650495565988524,\n",
       "  0.9384454877412624,\n",
       "  0.9297513475917232,\n",
       "  0.9683533298556772,\n",
       "  0.9746131107633456,\n",
       "  0.9436619718309859,\n",
       "  0.9544427056164145,\n",
       "  0.9688749782646496,\n",
       "  0.9613980177360459,\n",
       "  0.9547904712223961,\n",
       "  0.9721787515214745,\n",
       "  0.9337506520605112,\n",
       "  0.9754825247782994,\n",
       "  0.968527212658668,\n",
       "  0.9198400278212485,\n",
       "  0.9629629629629629,\n",
       "  0.9573987132672579,\n",
       "  0.9657450878108155,\n",
       "  0.97339593114241,\n",
       "  0.96452790818988,\n",
       "  0.9657450878108155,\n",
       "  0.9735698139454008,\n",
       "  0.9497478699356634,\n",
       "  0.9697443922796035,\n",
       "  0.9680055642496957,\n",
       "  0.9721787515214745,\n",
       "  0.970961571900539,\n",
       "  0.9780907668231612,\n",
       "  0.974960876369327,\n",
       "  0.9699182750825943,\n",
       "  0.9723526343244653,\n",
       "  0.9711354547035298,\n",
       "  0.9167101373674144,\n",
       "  0.9673100330377326,\n",
       "  0.9711354547035298,\n",
       "  0.9798295948530691,\n",
       "  0.9725265171274561,\n",
       "  0.9643540253868892,\n",
       "  0.9551382368283776,\n",
       "  0.9697443922796035,\n",
       "  0.9587897756911842,\n",
       "  0.9591375412971657,\n",
       "  0.970961571900539,\n",
       "  0.9648756737958616,\n",
       "  0.9720048687184838,\n",
       "  0.9810467744740046,\n",
       "  0.975830290384281,\n",
       "  0.9728742827334377,\n",
       "  0.9300991131977048,\n",
       "  0.9553121196313684,\n",
       "  0.9706138062945575,\n",
       "  0.9601808381151105,\n",
       "  0.9674839158407234,\n",
       "  0.9673100330377326,\n",
       "  0.9394887845592071,\n",
       "  0.9636584941749261,\n",
       "  0.9558337680403408,\n",
       "  0.9257520431229351,\n",
       "  0.9659189706138063,\n",
       "  0.970961571900539])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model = LstmClassifier(INPUT_DIM, N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model,\n",
    "            optimizer=torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            criterion=nn.CrossEntropyLoss(),\n",
    "            n_epoch=100,\n",
    "            batch_size=200,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            backup_name=\"lstm_action_classifier.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies = ([0.8735486226845841,\n",
    "  0.48130274497343034,\n",
    "  0.3702782436299719,\n",
    "  0.3491351202047991,\n",
    "  0.2769810041340675,\n",
    "  0.20154464583205914,\n",
    "  0.2445332597879415,\n",
    "  0.17224384997431086,\n",
    "  0.1885981674023096,\n",
    "  0.19384880530241444,\n",
    "  0.19337554822639866,\n",
    "  0.16536105377239416,\n",
    "  0.1571138994552154,\n",
    "  0.15273290199140158,\n",
    "  0.10815980746436514,\n",
    "  0.12133629483412643,\n",
    "  0.13833611768088946,\n",
    "  0.1293170387481046,\n",
    "  0.10733797376491747,\n",
    "  0.10590762594786797,\n",
    "  0.0918239034835805,\n",
    "  0.1047938061950121,\n",
    "  0.07058675783115197,\n",
    "  0.08766820735364987,\n",
    "  0.10277271190071632,\n",
    "  0.12322432884653176,\n",
    "  0.08019538524956828,\n",
    "  0.08205282129964776,\n",
    "  0.07426594978620334,\n",
    "  0.0755969595853548,\n",
    "  0.09760807308867492,\n",
    "  0.07207354542732404,\n",
    "  0.0975812500682325,\n",
    "  0.09537964315045605,\n",
    "  0.09366875433450493,\n",
    "  0.04696851966758542,\n",
    "  0.0512240380254569,\n",
    "  0.05459230239451631,\n",
    "  0.08089525441829507,\n",
    "  0.06131623744944018,\n",
    "  0.06199158364649636,\n",
    "  0.051481724839184166,\n",
    "  0.06538778840672245,\n",
    "  0.0629294374865063,\n",
    "  0.04425359480803349,\n",
    "  0.05855927577252546,\n",
    "  0.08699687327160525,\n",
    "  0.07487187519443134,\n",
    "  0.05951084620550851,\n",
    "  0.05211047360593793,\n",
    "  0.0654746382687632,\n",
    "  0.07681369263658207,\n",
    "  0.044812741151172154,\n",
    "  0.04169330514596971,\n",
    "  0.056397179180745924,\n",
    "  0.06540772032921424,\n",
    "  0.07633163423087057,\n",
    "  0.055633779746886776,\n",
    "  0.06838037114775641,\n",
    "  0.053590912112544256,\n",
    "  0.0628714390222539,\n",
    "  0.045393846311621903,\n",
    "  0.05751636609517408,\n",
    "  0.08074402875827821,\n",
    "  0.04321444615804029,\n",
    "  0.05079126861872594,\n",
    "  0.050103869856186006,\n",
    "  0.03507644276016325,\n",
    "  0.03688339269977289,\n",
    "  0.041359042481246576,\n",
    "  0.03544109247172077,\n",
    "  0.03485896200435596,\n",
    "  0.04210835051470699,\n",
    "  0.07559601816384749,\n",
    "  0.04357416177816813,\n",
    "  0.030860471100808837,\n",
    "  0.036541297235294604,\n",
    "  0.03869030817890365,\n",
    "  0.036192047033067894,\n",
    "  0.05795359878571838,\n",
    "  0.040659130261747875,\n",
    "  0.04481880535572273,\n",
    "  0.06331975865306445,\n",
    "  0.04256203787514518,\n",
    "  0.03762714751888046,\n",
    "  0.0230426168100116,\n",
    "  0.03029545205979716,\n",
    "  0.04663788862835142,\n",
    "  0.04618885129772497,\n",
    "  0.06090077068450866,\n",
    "  0.05378697729046914,\n",
    "  0.05114451131438682,\n",
    "  0.0467184322129791,\n",
    "  0.05389563671920387,\n",
    "  0.048165335642040105,\n",
    "  0.04513653215453111,\n",
    "  0.037053695992046956,\n",
    "  0.07355283905870348,\n",
    "  0.05021775581395428,\n",
    "  0.05387003534121017],\n",
    " [0.6489281767955801,\n",
    "  0.8121988950276243,\n",
    "  0.8585193370165746,\n",
    "  0.8681546961325967,\n",
    "  0.891889502762431,\n",
    "  0.9223425414364641,\n",
    "  0.906121546961326,\n",
    "  0.9344530386740332,\n",
    "  0.9319779005524862,\n",
    "  0.927646408839779,\n",
    "  0.925878453038674,\n",
    "  0.9390055248618785,\n",
    "  0.9413480662983426,\n",
    "  0.9398453038674033,\n",
    "  0.9611491712707182,\n",
    "  0.9558895027624309,\n",
    "  0.9517348066298342,\n",
    "  0.9534585635359116,\n",
    "  0.9620773480662983,\n",
    "  0.9622099447513812,\n",
    "  0.9653038674033149,\n",
    "  0.9622541436464088,\n",
    "  0.9747624309392265,\n",
    "  0.9688397790055249,\n",
    "  0.963403314917127,\n",
    "  0.9549613259668508,\n",
    "  0.9732154696132597,\n",
    "  0.9703867403314917,\n",
    "  0.974232044198895,\n",
    "  0.9735690607734807,\n",
    "  0.9656574585635359,\n",
    "  0.9750276243093923,\n",
    "  0.964817679558011,\n",
    "  0.9656132596685083,\n",
    "  0.9657016574585635,\n",
    "  0.9833370165745856,\n",
    "  0.9824530386740331,\n",
    "  0.9800220994475138,\n",
    "  0.9707845303867403,\n",
    "  0.9776795580110498,\n",
    "  0.9775027624309393,\n",
    "  0.9813480662983426,\n",
    "  0.9775911602209945,\n",
    "  0.9772375690607735,\n",
    "  0.9844861878453038,\n",
    "  0.9794475138121547,\n",
    "  0.9695469613259668,\n",
    "  0.9748066298342541,\n",
    "  0.9797569060773481,\n",
    "  0.981524861878453,\n",
    "  0.9777237569060774,\n",
    "  0.9734806629834254,\n",
    "  0.9838232044198895,\n",
    "  0.9848397790055249,\n",
    "  0.980243093922652,\n",
    "  0.9768839779005525,\n",
    "  0.9737016574585635,\n",
    "  0.9805966850828729,\n",
    "  0.9762651933701657,\n",
    "  0.9809060773480663,\n",
    "  0.978342541436464,\n",
    "  0.9840441988950276,\n",
    "  0.97953591160221,\n",
    "  0.9716685082872928,\n",
    "  0.9851491712707182,\n",
    "  0.9826298342541436,\n",
    "  0.9824972375690608,\n",
    "  0.9877127071823204,\n",
    "  0.9868729281767956,\n",
    "  0.9853259668508287,\n",
    "  0.9874475138121547,\n",
    "  0.9878453038674033,\n",
    "  0.9854143646408839,\n",
    "  0.9754696132596685,\n",
    "  0.9849723756906077,\n",
    "  0.9887734806629834,\n",
    "  0.9881546961325967,\n",
    "  0.9861215469613259,\n",
    "  0.9877569060773481,\n",
    "  0.9792707182320441,\n",
    "  0.9848397790055249,\n",
    "  0.984707182320442,\n",
    "  0.9776795580110498,\n",
    "  0.984707182320442,\n",
    "  0.9879779005524861,\n",
    "  0.9927513812154696,\n",
    "  0.9897458563535911,\n",
    "  0.9848839779005525,\n",
    "  0.9849281767955801,\n",
    "  0.9796685082872928,\n",
    "  0.9809502762430939,\n",
    "  0.9824530386740331,\n",
    "  0.9832044198895028,\n",
    "  0.9803314917127072,\n",
    "  0.9834254143646409,\n",
    "  0.9847513812154696,\n",
    "  0.9872265193370166,\n",
    "  0.9749834254143647,\n",
    "  0.9823204419889503,\n",
    "  0.982718232044199],\n",
    " [0.6211841620039762,\n",
    "  0.45876633668316125,\n",
    "  0.5034662859063835,\n",
    "  0.27011767895693073,\n",
    "  0.313655665158899,\n",
    "  0.20805899514211246,\n",
    "  0.3991670820106114,\n",
    "  0.32158903264995403,\n",
    "  0.24071570668090958,\n",
    "  0.2567666179594932,\n",
    "  0.3852716677423312,\n",
    "  0.4597050237921575,\n",
    "  0.17627169369967327,\n",
    "  0.23679502981565434,\n",
    "  0.17220754785561754,\n",
    "  0.18002146034804078,\n",
    "  0.22931813937735357,\n",
    "  0.16827500484832936,\n",
    "  0.15206738518692042,\n",
    "  0.14282209915193997,\n",
    "  0.18685480398813387,\n",
    "  0.11685570925779426,\n",
    "  0.20067864179665781,\n",
    "  0.1411727972821085,\n",
    "  0.1530777178196244,\n",
    "  0.13876111770846125,\n",
    "  0.14196548254774952,\n",
    "  0.2708777552003365,\n",
    "  0.11546770097299551,\n",
    "  0.3046354775860649,\n",
    "  0.10854558102802168,\n",
    "  0.07933161259128273,\n",
    "  0.12404691129011139,\n",
    "  0.38784327351626957,\n",
    "  0.07989218154476951,\n",
    "  0.17622217972792195,\n",
    "  0.09162371938435548,\n",
    "  0.13405118411540942,\n",
    "  0.10392435214123062,\n",
    "  0.10115272597895769,\n",
    "  0.09087322915753134,\n",
    "  0.1590506696334804,\n",
    "  0.22317246896386603,\n",
    "  0.08073770713501655,\n",
    "  0.058100116569523644,\n",
    "  0.14888879645115743,\n",
    "  0.13044291378562928,\n",
    "  0.07736094586014841,\n",
    "  0.11220023536174023,\n",
    "  0.13659915878864518,\n",
    "  0.07129768802977254,\n",
    "  0.20124599370539134,\n",
    "  0.07354985906765003,\n",
    "  0.08835384215030306,\n",
    "  0.21074753585173564,\n",
    "  0.09616019399096185,\n",
    "  0.11539241931760864,\n",
    "  0.120142063001071,\n",
    "  0.0775641988276421,\n",
    "  0.09695740938079764,\n",
    "  0.09058009887954965,\n",
    "  0.06536958760441548,\n",
    "  0.14220434166765117,\n",
    "  0.07930320302022702,\n",
    "  0.08943844097148232,\n",
    "  0.06493218757685769,\n",
    "  0.08265407143692353,\n",
    "  0.0510503365727251,\n",
    "  0.06409464690922057,\n",
    "  0.0902351198168243,\n",
    "  0.06698930089201968,\n",
    "  0.07941066838737426,\n",
    "  0.27309605946031246,\n",
    "  0.10141229745728485,\n",
    "  0.08438794725363463,\n",
    "  0.06564738088682306,\n",
    "  0.07934415308124455,\n",
    "  0.11631594676814944,\n",
    "  0.11903673065906457,\n",
    "  0.09058221255546416,\n",
    "  0.12191057274405673,\n",
    "  0.1233115003656721,\n",
    "  0.10492566676817347,\n",
    "  0.11040246008239304,\n",
    "  0.0784957536592911,\n",
    "  0.05796452499581156,\n",
    "  0.06605394684588234,\n",
    "  0.08701971364890956,\n",
    "  0.23499713032428876,\n",
    "  0.14679802382028134,\n",
    "  0.0931847726414798,\n",
    "  0.1281534519503502,\n",
    "  0.09721326736259316,\n",
    "  0.0960857172550666,\n",
    "  0.1704642465234064,\n",
    "  0.11685481813445657,\n",
    "  0.14966437045596195,\n",
    "  0.22490323309782892,\n",
    "  0.09092609374476246,\n",
    "  0.08504296933522287],\n",
    " [0.7440445139975657,\n",
    "  0.8125543383759346,\n",
    "  0.8101199791340636,\n",
    "  0.9012345679012346,\n",
    "  0.869414014953921,\n",
    "  0.9231438010780734,\n",
    "  0.8497652582159625,\n",
    "  0.892018779342723,\n",
    "  0.9128847157016171,\n",
    "  0.9021039819161885,\n",
    "  0.8753260302556077,\n",
    "  0.8575899843505478,\n",
    "  0.9285341679707877,\n",
    "  0.9189706138062945,\n",
    "  0.9384454877412624,\n",
    "  0.9398365501651886,\n",
    "  0.9163623717614328,\n",
    "  0.94679186228482,\n",
    "  0.9419231438010781,\n",
    "  0.942792557816032,\n",
    "  0.9297513475917232,\n",
    "  0.952529994783516,\n",
    "  0.9400104329681794,\n",
    "  0.9497478699356634,\n",
    "  0.9521822291775344,\n",
    "  0.9539210572074421,\n",
    "  0.9410537297861241,\n",
    "  0.9031472787341331,\n",
    "  0.9601808381151105,\n",
    "  0.8941053729786124,\n",
    "  0.9612241349330551,\n",
    "  0.9688749782646496,\n",
    "  0.9575725960702487,\n",
    "  0.8626325856372805,\n",
    "  0.9680055642496957,\n",
    "  0.9514866979655712,\n",
    "  0.9640062597809077,\n",
    "  0.952529994783516,\n",
    "  0.9575725960702487,\n",
    "  0.9641801425838985,\n",
    "  0.9650495565988524,\n",
    "  0.9384454877412624,\n",
    "  0.9297513475917232,\n",
    "  0.9683533298556772,\n",
    "  0.9746131107633456,\n",
    "  0.9436619718309859,\n",
    "  0.9544427056164145,\n",
    "  0.9688749782646496,\n",
    "  0.9613980177360459,\n",
    "  0.9547904712223961,\n",
    "  0.9721787515214745,\n",
    "  0.9337506520605112,\n",
    "  0.9754825247782994,\n",
    "  0.968527212658668,\n",
    "  0.9198400278212485,\n",
    "  0.9629629629629629,\n",
    "  0.9573987132672579,\n",
    "  0.9657450878108155,\n",
    "  0.97339593114241,\n",
    "  0.96452790818988,\n",
    "  0.9657450878108155,\n",
    "  0.9735698139454008,\n",
    "  0.9497478699356634,\n",
    "  0.9697443922796035,\n",
    "  0.9680055642496957,\n",
    "  0.9721787515214745,\n",
    "  0.970961571900539,\n",
    "  0.9780907668231612,\n",
    "  0.974960876369327,\n",
    "  0.9699182750825943,\n",
    "  0.9723526343244653,\n",
    "  0.9711354547035298,\n",
    "  0.9167101373674144,\n",
    "  0.9673100330377326,\n",
    "  0.9711354547035298,\n",
    "  0.9798295948530691,\n",
    "  0.9725265171274561,\n",
    "  0.9643540253868892,\n",
    "  0.9551382368283776,\n",
    "  0.9697443922796035,\n",
    "  0.9587897756911842,\n",
    "  0.9591375412971657,\n",
    "  0.970961571900539,\n",
    "  0.9648756737958616,\n",
    "  0.9720048687184838,\n",
    "  0.9810467744740046,\n",
    "  0.975830290384281,\n",
    "  0.9728742827334377,\n",
    "  0.9300991131977048,\n",
    "  0.9553121196313684,\n",
    "  0.9706138062945575,\n",
    "  0.9601808381151105,\n",
    "  0.9674839158407234,\n",
    "  0.9673100330377326,\n",
    "  0.9394887845592071,\n",
    "  0.9636584941749261,\n",
    "  0.9558337680403408,\n",
    "  0.9257520431229351,\n",
    "  0.9659189706138063,\n",
    "  0.970961571900539])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}