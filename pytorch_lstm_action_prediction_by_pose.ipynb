{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit85ef42dc311540dbbdc2b3827db2c57f",
   "display_name": "Python 3.6.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "DATASET_PATH = \"poses/\"\n",
    "\n",
    "X_train_path = \"shuffled_x_train_data\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "y_train_path = \"shuffled_y_train_data\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 6\n",
    "num_epochs = 10\n",
    "train_batch_size = 125\n",
    "test_batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 36\n",
    "sequence_length = 32\n",
    "hidden_size = 128\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [  \"j0_x\",  \"j0_y\", \"j1_x\", \"j1_y\" , \"j2_x\", \"j2_y\", \"j3_x\", \"j3_y\", \"j4_x\", \"j4_y\", \"j5_x\", \"j5_y\", \"j6_x\", \"j6_y\", \"j7_x\", \"j7_y\", \"j8_x\", \"j8_y\", \"j9_x\", \"j9_y\", \"j10_x\", \"j10_y\", \"j11_x\", \"j11_y\", \"j12_x\", \"j12_y\", \"j13_x\", \"j13_y\", 'j14_x', \"j14_y\", \"j15_x\", \"j15_y\", \"j16_x\", \"j16_y\", \"j17_x\", \"j17_y\" ]\n",
    "# x_train_data = pd.read_csv(X_train_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "x_train_data = pd.read_csv(X_train_path, sep=\"\\t\", dtype=np.float32)\n",
    "x_test_data = pd.read_csv(X_test_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "# y_train_data = p|d.read_csv(y_train_path, names=[\"labels\"], dtype=np.int_)\n",
    "y_train_data = pd.read_csv(y_train_path, sep=\"\\t\", dtype=np.int_)\n",
    "y_test_data = pd.read_csv(y_test_path, names=[\"labels\"], dtype=np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_x_train_data = x_train_data\n",
    "shuffled_y_train_data = y_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    for i, column in enumerate(data):\n",
    "        if i % 2 == 0:\n",
    "            data[column] = data[column] / 640\n",
    "        else:\n",
    "            data[column] = data[column] / 480\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       j0_x      j0_y      j1_x      j1_y      j2_x      j2_y      j3_x  \\\n",
       "0  0.480608  0.339533  0.499006  0.429050  0.458230  0.426417  0.445991   \n",
       "1  0.480573  0.339540  0.499003  0.429056  0.458214  0.426448  0.445981   \n",
       "2  0.478591  0.339481  0.498986  0.429035  0.456236  0.426415  0.445988   \n",
       "3  0.478508  0.336883  0.498959  0.426423  0.456230  0.426333  0.445983   \n",
       "4  0.476562  0.336775  0.497027  0.426348  0.454209  0.423727  0.445997   \n",
       "\n",
       "       j3_y      j4_x      j4_y  ...     j13_x     j13_y     j14_x     j14_y  \\\n",
       "0  0.521419  0.433775  0.597352  ...  0.527416  0.858235  0.476553  0.333940   \n",
       "1  0.521417  0.433775  0.594800  ...  0.527414  0.858202  0.476514  0.333948   \n",
       "2  0.524065  0.433777  0.597444  ...  0.527422  0.858233  0.472530  0.333890   \n",
       "3  0.521427  0.431887  0.597506  ...  0.527445  0.858206  0.470505  0.331300   \n",
       "4  0.521408  0.431880  0.597565  ...  0.527423  0.858217  0.470452  0.328621   \n",
       "\n",
       "      j15_x     j15_y  j16_x  j16_y     j17_x     j17_y  \n",
       "0  0.492905  0.333894    0.0    0.0  0.515238  0.336773  \n",
       "1  0.492866  0.333892    0.0    0.0  0.513323  0.336781  \n",
       "2  0.490823  0.333875    0.0    0.0  0.513273  0.336835  \n",
       "3  0.488778  0.331308    0.0    0.0  0.513252  0.336785  \n",
       "4  0.486744  0.331202    0.0    0.0  0.513173  0.336777  \n",
       "\n",
       "[5 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j0_x</th>\n      <th>j0_y</th>\n      <th>j1_x</th>\n      <th>j1_y</th>\n      <th>j2_x</th>\n      <th>j2_y</th>\n      <th>j3_x</th>\n      <th>j3_y</th>\n      <th>j4_x</th>\n      <th>j4_y</th>\n      <th>...</th>\n      <th>j13_x</th>\n      <th>j13_y</th>\n      <th>j14_x</th>\n      <th>j14_y</th>\n      <th>j15_x</th>\n      <th>j15_y</th>\n      <th>j16_x</th>\n      <th>j16_y</th>\n      <th>j17_x</th>\n      <th>j17_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.480608</td>\n      <td>0.339533</td>\n      <td>0.499006</td>\n      <td>0.429050</td>\n      <td>0.458230</td>\n      <td>0.426417</td>\n      <td>0.445991</td>\n      <td>0.521419</td>\n      <td>0.433775</td>\n      <td>0.597352</td>\n      <td>...</td>\n      <td>0.527416</td>\n      <td>0.858235</td>\n      <td>0.476553</td>\n      <td>0.333940</td>\n      <td>0.492905</td>\n      <td>0.333894</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.515238</td>\n      <td>0.336773</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.480573</td>\n      <td>0.339540</td>\n      <td>0.499003</td>\n      <td>0.429056</td>\n      <td>0.458214</td>\n      <td>0.426448</td>\n      <td>0.445981</td>\n      <td>0.521417</td>\n      <td>0.433775</td>\n      <td>0.594800</td>\n      <td>...</td>\n      <td>0.527414</td>\n      <td>0.858202</td>\n      <td>0.476514</td>\n      <td>0.333948</td>\n      <td>0.492866</td>\n      <td>0.333892</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.513323</td>\n      <td>0.336781</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.478591</td>\n      <td>0.339481</td>\n      <td>0.498986</td>\n      <td>0.429035</td>\n      <td>0.456236</td>\n      <td>0.426415</td>\n      <td>0.445988</td>\n      <td>0.524065</td>\n      <td>0.433777</td>\n      <td>0.597444</td>\n      <td>...</td>\n      <td>0.527422</td>\n      <td>0.858233</td>\n      <td>0.472530</td>\n      <td>0.333890</td>\n      <td>0.490823</td>\n      <td>0.333875</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.513273</td>\n      <td>0.336835</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.478508</td>\n      <td>0.336883</td>\n      <td>0.498959</td>\n      <td>0.426423</td>\n      <td>0.456230</td>\n      <td>0.426333</td>\n      <td>0.445983</td>\n      <td>0.521427</td>\n      <td>0.431887</td>\n      <td>0.597506</td>\n      <td>...</td>\n      <td>0.527445</td>\n      <td>0.858206</td>\n      <td>0.470505</td>\n      <td>0.331300</td>\n      <td>0.488778</td>\n      <td>0.331308</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.513252</td>\n      <td>0.336785</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.476562</td>\n      <td>0.336775</td>\n      <td>0.497027</td>\n      <td>0.426348</td>\n      <td>0.454209</td>\n      <td>0.423727</td>\n      <td>0.445997</td>\n      <td>0.521408</td>\n      <td>0.431880</td>\n      <td>0.597565</td>\n      <td>...</td>\n      <td>0.527423</td>\n      <td>0.858217</td>\n      <td>0.470452</td>\n      <td>0.328621</td>\n      <td>0.486744</td>\n      <td>0.331202</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.513173</td>\n      <td>0.336777</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# normalize(x_train_data)\n",
    "normalize(x_test_data).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      labels\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "...      ...\n",
       "5746       5\n",
       "5747       5\n",
       "5748       5\n",
       "5749       5\n",
       "5750       5\n",
       "\n",
       "[5751 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5746</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5747</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5748</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5749</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5750</th>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5751 rows Ã— 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# y_train_data = y_train_data - 1\n",
    "y_test_data = y_test_data - 1\n",
    "y_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         j0_x      j0_y      j1_x      j1_y      j2_x      j2_y      j3_x  \\\n",
       "0    0.462366  0.336623  0.480770  0.423777  0.439916  0.423683  0.429683   \n",
       "1    0.462273  0.336667  0.480756  0.423767  0.439889  0.423719  0.429670   \n",
       "2    0.458275  0.336708  0.480711  0.423771  0.437856  0.423713  0.429666   \n",
       "3    0.456216  0.336788  0.480655  0.426412  0.435809  0.426373  0.429689   \n",
       "4    0.450098  0.350250  0.478691  0.437156  0.431850  0.439808  0.429703   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "124  0.482669  0.461650  0.507156  0.543058  0.462413  0.537667  0.456241   \n",
       "125  0.482670  0.458877  0.509088  0.537698  0.464319  0.535040  0.454194   \n",
       "126  0.480758  0.434465  0.507131  0.513310  0.464323  0.513231  0.454150   \n",
       "127  0.482702  0.399183  0.511097  0.486019  0.466362  0.485990  0.448037   \n",
       "128  0.454184  0.358477  0.478628  0.445327  0.431692  0.445331  0.425677   \n",
       "\n",
       "         j3_y      j4_x      j4_y  ...     j13_x     j13_y     j14_x  \\\n",
       "0    0.524087  0.417491  0.610944  ...  0.513325  0.858171  0.460228   \n",
       "1    0.524113  0.417486  0.608252  ...  0.513270  0.855479  0.458206   \n",
       "2    0.524148  0.417597  0.602860  ...  0.513316  0.858221  0.452147   \n",
       "3    0.529404  0.421553  0.602840  ...  0.513248  0.858223  0.450158   \n",
       "4    0.532210  0.429692  0.613710  ...  0.515272  0.858092  0.448084   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "124  0.621944  0.448055  0.700533  ...  0.539755  0.836385  0.478620   \n",
       "125  0.624552  0.445911  0.700531  ...  0.539711  0.833735  0.478647   \n",
       "126  0.597542  0.437884  0.665379  ...  0.539739  0.833608  0.480572   \n",
       "127  0.575627  0.427670  0.640977  ...  0.545772  0.836533  0.480677   \n",
       "128  0.540433  0.417520  0.619196  ...  0.521392  0.858212  0.452156   \n",
       "\n",
       "        j14_y     j15_x     j15_y     j16_x     j16_y     j17_x     j17_y  \n",
       "0    0.325875  0.476566  0.325871  0.000000  0.000000  0.497005  0.336733  \n",
       "1    0.328513  0.474541  0.328554  0.000000  0.000000  0.496912  0.336779  \n",
       "2    0.328513  0.468434  0.328571  0.000000  0.000000  0.494963  0.336831  \n",
       "3    0.328598  0.466386  0.331219  0.000000  0.000000  0.494875  0.336869  \n",
       "4    0.336658  0.464270  0.336669  0.000000  0.000000  0.492816  0.347583  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "124  0.448131  0.494927  0.445487  0.466450  0.467079  0.519323  0.464240  \n",
       "125  0.445417  0.494953  0.445260  0.472498  0.467006  0.521408  0.458910  \n",
       "126  0.421023  0.494912  0.423521  0.474497  0.442490  0.519367  0.437152  \n",
       "127  0.388329  0.494923  0.385629  0.000000  0.000000  0.517366  0.399210  \n",
       "128  0.350315  0.464437  0.352979  0.000000  0.000000  0.492852  0.358504  \n",
       "\n",
       "[129 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j0_x</th>\n      <th>j0_y</th>\n      <th>j1_x</th>\n      <th>j1_y</th>\n      <th>j2_x</th>\n      <th>j2_y</th>\n      <th>j3_x</th>\n      <th>j3_y</th>\n      <th>j4_x</th>\n      <th>j4_y</th>\n      <th>...</th>\n      <th>j13_x</th>\n      <th>j13_y</th>\n      <th>j14_x</th>\n      <th>j14_y</th>\n      <th>j15_x</th>\n      <th>j15_y</th>\n      <th>j16_x</th>\n      <th>j16_y</th>\n      <th>j17_x</th>\n      <th>j17_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.462366</td>\n      <td>0.336623</td>\n      <td>0.480770</td>\n      <td>0.423777</td>\n      <td>0.439916</td>\n      <td>0.423683</td>\n      <td>0.429683</td>\n      <td>0.524087</td>\n      <td>0.417491</td>\n      <td>0.610944</td>\n      <td>...</td>\n      <td>0.513325</td>\n      <td>0.858171</td>\n      <td>0.460228</td>\n      <td>0.325875</td>\n      <td>0.476566</td>\n      <td>0.325871</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.497005</td>\n      <td>0.336733</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.462273</td>\n      <td>0.336667</td>\n      <td>0.480756</td>\n      <td>0.423767</td>\n      <td>0.439889</td>\n      <td>0.423719</td>\n      <td>0.429670</td>\n      <td>0.524113</td>\n      <td>0.417486</td>\n      <td>0.608252</td>\n      <td>...</td>\n      <td>0.513270</td>\n      <td>0.855479</td>\n      <td>0.458206</td>\n      <td>0.328513</td>\n      <td>0.474541</td>\n      <td>0.328554</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.496912</td>\n      <td>0.336779</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.458275</td>\n      <td>0.336708</td>\n      <td>0.480711</td>\n      <td>0.423771</td>\n      <td>0.437856</td>\n      <td>0.423713</td>\n      <td>0.429666</td>\n      <td>0.524148</td>\n      <td>0.417597</td>\n      <td>0.602860</td>\n      <td>...</td>\n      <td>0.513316</td>\n      <td>0.858221</td>\n      <td>0.452147</td>\n      <td>0.328513</td>\n      <td>0.468434</td>\n      <td>0.328571</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494963</td>\n      <td>0.336831</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.456216</td>\n      <td>0.336788</td>\n      <td>0.480655</td>\n      <td>0.426412</td>\n      <td>0.435809</td>\n      <td>0.426373</td>\n      <td>0.429689</td>\n      <td>0.529404</td>\n      <td>0.421553</td>\n      <td>0.602840</td>\n      <td>...</td>\n      <td>0.513248</td>\n      <td>0.858223</td>\n      <td>0.450158</td>\n      <td>0.328598</td>\n      <td>0.466386</td>\n      <td>0.331219</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494875</td>\n      <td>0.336869</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.450098</td>\n      <td>0.350250</td>\n      <td>0.478691</td>\n      <td>0.437156</td>\n      <td>0.431850</td>\n      <td>0.439808</td>\n      <td>0.429703</td>\n      <td>0.532210</td>\n      <td>0.429692</td>\n      <td>0.613710</td>\n      <td>...</td>\n      <td>0.515272</td>\n      <td>0.858092</td>\n      <td>0.448084</td>\n      <td>0.336658</td>\n      <td>0.464270</td>\n      <td>0.336669</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.492816</td>\n      <td>0.347583</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>0.482669</td>\n      <td>0.461650</td>\n      <td>0.507156</td>\n      <td>0.543058</td>\n      <td>0.462413</td>\n      <td>0.537667</td>\n      <td>0.456241</td>\n      <td>0.621944</td>\n      <td>0.448055</td>\n      <td>0.700533</td>\n      <td>...</td>\n      <td>0.539755</td>\n      <td>0.836385</td>\n      <td>0.478620</td>\n      <td>0.448131</td>\n      <td>0.494927</td>\n      <td>0.445487</td>\n      <td>0.466450</td>\n      <td>0.467079</td>\n      <td>0.519323</td>\n      <td>0.464240</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>0.482670</td>\n      <td>0.458877</td>\n      <td>0.509088</td>\n      <td>0.537698</td>\n      <td>0.464319</td>\n      <td>0.535040</td>\n      <td>0.454194</td>\n      <td>0.624552</td>\n      <td>0.445911</td>\n      <td>0.700531</td>\n      <td>...</td>\n      <td>0.539711</td>\n      <td>0.833735</td>\n      <td>0.478647</td>\n      <td>0.445417</td>\n      <td>0.494953</td>\n      <td>0.445260</td>\n      <td>0.472498</td>\n      <td>0.467006</td>\n      <td>0.521408</td>\n      <td>0.458910</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>0.480758</td>\n      <td>0.434465</td>\n      <td>0.507131</td>\n      <td>0.513310</td>\n      <td>0.464323</td>\n      <td>0.513231</td>\n      <td>0.454150</td>\n      <td>0.597542</td>\n      <td>0.437884</td>\n      <td>0.665379</td>\n      <td>...</td>\n      <td>0.539739</td>\n      <td>0.833608</td>\n      <td>0.480572</td>\n      <td>0.421023</td>\n      <td>0.494912</td>\n      <td>0.423521</td>\n      <td>0.474497</td>\n      <td>0.442490</td>\n      <td>0.519367</td>\n      <td>0.437152</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>0.482702</td>\n      <td>0.399183</td>\n      <td>0.511097</td>\n      <td>0.486019</td>\n      <td>0.466362</td>\n      <td>0.485990</td>\n      <td>0.448037</td>\n      <td>0.575627</td>\n      <td>0.427670</td>\n      <td>0.640977</td>\n      <td>...</td>\n      <td>0.545772</td>\n      <td>0.836533</td>\n      <td>0.480677</td>\n      <td>0.388329</td>\n      <td>0.494923</td>\n      <td>0.385629</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.517366</td>\n      <td>0.399210</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>0.454184</td>\n      <td>0.358477</td>\n      <td>0.478628</td>\n      <td>0.445327</td>\n      <td>0.431692</td>\n      <td>0.445331</td>\n      <td>0.425677</td>\n      <td>0.540433</td>\n      <td>0.417520</td>\n      <td>0.619196</td>\n      <td>...</td>\n      <td>0.521392</td>\n      <td>0.858212</td>\n      <td>0.452156</td>\n      <td>0.350315</td>\n      <td>0.464437</td>\n      <td>0.352979</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.492852</td>\n      <td>0.358504</td>\n    </tr>\n  </tbody>\n</table>\n<p>129 rows Ã— 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "shuffled_y_train_data = pd.DataFrame()\n",
    "shuffled_y_train_data = shuffled_y_train_data.append(x_train_data.iloc[0:32], ignore_index=True)\n",
    "shuffled_y_train_data = shuffled_y_train_data.append(x_train_data.iloc[1*32:2*64], ignore_index=True)\n",
    "shuffled_y_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# def shuffle_data(x_train_data, y_train_data):\n",
    "length = y_train_data.shape[0]\n",
    "shuffled_indexes = [i for i in range(length)]\n",
    "shuffle(shuffled_indexes)\n",
    "shuffled_y_train_data = pd.DataFrame()\n",
    "shuffled_x_train_data = pd.DataFrame()\n",
    "for i, index in enumerate(shuffled_indexes): \n",
    "    shuffled_y_train_data = shuffled_y_train_data.append(y_train_data.iloc[[index]], ignore_index=True)\n",
    "    shuffled_x_train_data = shuffled_x_train_data.append(x_train_data.iloc[index*32:index*32+32], ignore_index=True)\n",
    "    if (i % 1000 == 0):\n",
    "        print(f\"{i} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_x_train_data.to_csv(\"shuffled_x_train_data\", sep='\\t', index=False)\n",
    "shuffled_y_train_data.to_csv(\"shuffled_y_train_data\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(724000, 36)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "shuffled_x_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:100\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(x_seq, y_seq, seq_size, batch_size):\n",
    "    for batch_pos in range(0, len(y_seq), batch_size):\n",
    "        x_batch = list()\n",
    "        for pos in range(batch_size):\n",
    "            # print(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size])\n",
    "            x_batch.append(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size].values)\n",
    "        \n",
    "        yield torch.tensor(x_batch), torch.flatten(torch.tensor(y_seq.iloc[batch_pos:batch_pos + batch_size].values))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/10], Step [10/181], Loss: 1.7741\n",
      "Epoch [1/10], Step [20/181], Loss: 1.7528\n",
      "Epoch [1/10], Step [30/181], Loss: 1.7831\n",
      "Epoch [1/10], Step [40/181], Loss: 1.7617\n",
      "Epoch [1/10], Step [50/181], Loss: 1.7554\n",
      "Epoch [1/10], Step [60/181], Loss: 1.7393\n",
      "Epoch [1/10], Step [70/181], Loss: 1.7684\n",
      "Epoch [1/10], Step [80/181], Loss: 1.7319\n",
      "Epoch [1/10], Step [90/181], Loss: 1.7370\n",
      "Epoch [1/10], Step [100/181], Loss: 1.7518\n",
      "Epoch [1/10], Step [110/181], Loss: 1.7140\n",
      "Epoch [1/10], Step [120/181], Loss: 1.7834\n",
      "Epoch [1/10], Step [130/181], Loss: 1.7237\n",
      "Epoch [1/10], Step [140/181], Loss: 1.7397\n",
      "Epoch [1/10], Step [150/181], Loss: 1.7562\n",
      "Epoch [1/10], Step [160/181], Loss: 1.6981\n",
      "Epoch [1/10], Step [170/181], Loss: 1.7227\n",
      "Epoch [1/10], Step [180/181], Loss: 1.6970\n",
      "Accuracy: 21.91381215469613 %\n",
      "Accuracy: 22.830812032689966 %\n",
      "Epoch [2/10], Step [10/181], Loss: 1.7786\n",
      "Epoch [2/10], Step [20/181], Loss: 1.7449\n",
      "Epoch [2/10], Step [30/181], Loss: 1.7761\n",
      "Epoch [2/10], Step [40/181], Loss: 1.7594\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7c6b76008246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_total_steps = int(shuffled_y_train_data.shape[0] / train_batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(chunker(shuffled_x_train_data, shuffled_y_train_data, sequence_length, train_batch_size)):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        y_batch = y_batch.to(device)\n",
    "        x_batch = x_batch.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "    acc = 100.0 * correct / y_train_data.shape[0]\n",
    "    print (f'Accuracy: {acc} %')\n",
    "    # Test the model\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for i, (x_batch, y_batch) in enumerate(chunker(x_test_data, y_test_data, sequence_length, 5751)):  \n",
    "            # origin shape: [N, 1, 28, 28]\n",
    "            # resized: [N, 28, 28]\n",
    "            y_batch = y_batch.to(device)\n",
    "            x_batch = x_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            \n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "        acc = 100.0 * correct / y_test_data.shape[0]\n",
    "        print(f'Accuracy: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 10000 test images: 7.7030081724917405 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(chunker(x_test_data, y_test_data, sequence_length, 5751)):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        y_batch = y_batch.to(device)\n",
    "        x_batch = x_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    acc = 100.0 * correct / y_test_data.shape[0]\n",
    "    print(f'Accuracy: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}