{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit29e064361c954e90adf267cf1683fa25",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "DATASET_PATH = \"poses_dataset/\"\n",
    "\n",
    "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 6\n",
    "num_epochs = 100\n",
    "train_batch_size = 125\n",
    "test_batch_size = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 36\n",
    "sequence_length = 32\n",
    "hidden_size = 128\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [  \"j0_x\",  \"j0_y\", \"j1_x\", \"j1_y\" , \"j2_x\", \"j2_y\", \"j3_x\", \"j3_y\", \"j4_x\", \"j4_y\", \"j5_x\", \"j5_y\", \"j6_x\", \"j6_y\", \"j7_x\", \"j7_y\", \"j8_x\", \"j8_y\", \"j9_x\", \"j9_y\", \"j10_x\", \"j10_y\", \"j11_x\", \"j11_y\", \"j12_x\", \"j12_y\", \"j13_x\", \"j13_y\", 'j14_x', \"j14_y\", \"j15_x\", \"j15_y\", \"j16_x\", \"j16_y\", \"j17_x\", \"j17_y\" ]\n",
    "x_train_data = pd.read_csv(X_train_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "x_test_data = pd.read_csv(X_test_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "y_train_data = pd.read_csv(y_train_path, names=[\"labels\"], dtype=np.int_)\n",
    "y_test_data = pd.read_csv(y_test_path, names=[\"labels\"], dtype=np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    for i, column in enumerate(data):\n",
    "        if i % 2 == 0:\n",
    "            data[column] = data[column] / 640\n",
    "        else:\n",
    "            data[column] = data[column] / 480\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            j0_x      j0_y      j1_x      j1_y      j2_x      j2_y      j3_x  \\\n",
       "0       0.462366  0.336623  0.480770  0.423777  0.439916  0.423683  0.429683   \n",
       "1       0.462273  0.336667  0.480756  0.423767  0.439889  0.423719  0.429670   \n",
       "2       0.458275  0.336708  0.480711  0.423771  0.437856  0.423713  0.429666   \n",
       "3       0.456216  0.336788  0.480655  0.426412  0.435809  0.426373  0.429689   \n",
       "4       0.450098  0.350250  0.478691  0.437156  0.431850  0.439808  0.429703   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "723995  0.572323  0.249740  0.545817  0.331265  0.513305  0.334060  0.496944   \n",
       "723996  0.574264  0.247185  0.545855  0.331294  0.513342  0.334083  0.496923   \n",
       "723997  0.572278  0.247181  0.545858  0.331319  0.515255  0.334127  0.496880   \n",
       "723998  0.572309  0.247146  0.545872  0.333883  0.515258  0.336619  0.496959   \n",
       "723999  0.574313  0.247090  0.545845  0.333906  0.513322  0.336681  0.497050   \n",
       "\n",
       "            j3_y      j4_x      j4_y  ...     j13_x     j13_y     j14_x  \\\n",
       "0       0.524087  0.417491  0.610944  ...  0.513325  0.858171  0.460228   \n",
       "1       0.524113  0.417486  0.608252  ...  0.513270  0.855479  0.458206   \n",
       "2       0.524148  0.417597  0.602860  ...  0.513316  0.858221  0.452147   \n",
       "3       0.529404  0.421553  0.602840  ...  0.513248  0.858223  0.450158   \n",
       "4       0.532210  0.429692  0.613710  ...  0.515272  0.858092  0.448084   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "723995  0.439946  0.515239  0.507831  ...  0.557984  0.743981  0.560053   \n",
       "723996  0.439842  0.509138  0.507875  ...  0.556023  0.743962  0.560064   \n",
       "723997  0.442504  0.503084  0.521377  ...  0.556000  0.744010  0.560003   \n",
       "723998  0.442683  0.496922  0.537717  ...  0.556006  0.744062  0.560016   \n",
       "723999  0.442677  0.494947  0.534938  ...  0.556038  0.744019  0.560069   \n",
       "\n",
       "           j14_y     j15_x     j15_y     j16_x     j16_y     j17_x     j17_y  \n",
       "0       0.325875  0.476566  0.325871  0.000000  0.000000  0.497005  0.336733  \n",
       "1       0.328513  0.474541  0.328554  0.000000  0.000000  0.496912  0.336779  \n",
       "2       0.328513  0.468434  0.328571  0.000000  0.000000  0.494963  0.336831  \n",
       "3       0.328598  0.466386  0.331219  0.000000  0.000000  0.494875  0.336869  \n",
       "4       0.336658  0.464270  0.336669  0.000000  0.000000  0.492816  0.347583  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "723995  0.241677  0.574331  0.236296  0.531556  0.249948  0.000000  0.000000  \n",
       "723996  0.239012  0.574341  0.233590  0.531623  0.249950  0.000000  0.000000  \n",
       "723997  0.239012  0.574309  0.233606  0.531602  0.249977  0.000000  0.000000  \n",
       "723998  0.238944  0.574327  0.233575  0.531600  0.249969  0.000000  0.000000  \n",
       "723999  0.236244  0.574359  0.230927  0.531583  0.249944  0.000000  0.000000  \n",
       "\n",
       "[724000 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j0_x</th>\n      <th>j0_y</th>\n      <th>j1_x</th>\n      <th>j1_y</th>\n      <th>j2_x</th>\n      <th>j2_y</th>\n      <th>j3_x</th>\n      <th>j3_y</th>\n      <th>j4_x</th>\n      <th>j4_y</th>\n      <th>...</th>\n      <th>j13_x</th>\n      <th>j13_y</th>\n      <th>j14_x</th>\n      <th>j14_y</th>\n      <th>j15_x</th>\n      <th>j15_y</th>\n      <th>j16_x</th>\n      <th>j16_y</th>\n      <th>j17_x</th>\n      <th>j17_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.462366</td>\n      <td>0.336623</td>\n      <td>0.480770</td>\n      <td>0.423777</td>\n      <td>0.439916</td>\n      <td>0.423683</td>\n      <td>0.429683</td>\n      <td>0.524087</td>\n      <td>0.417491</td>\n      <td>0.610944</td>\n      <td>...</td>\n      <td>0.513325</td>\n      <td>0.858171</td>\n      <td>0.460228</td>\n      <td>0.325875</td>\n      <td>0.476566</td>\n      <td>0.325871</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.497005</td>\n      <td>0.336733</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.462273</td>\n      <td>0.336667</td>\n      <td>0.480756</td>\n      <td>0.423767</td>\n      <td>0.439889</td>\n      <td>0.423719</td>\n      <td>0.429670</td>\n      <td>0.524113</td>\n      <td>0.417486</td>\n      <td>0.608252</td>\n      <td>...</td>\n      <td>0.513270</td>\n      <td>0.855479</td>\n      <td>0.458206</td>\n      <td>0.328513</td>\n      <td>0.474541</td>\n      <td>0.328554</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.496912</td>\n      <td>0.336779</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.458275</td>\n      <td>0.336708</td>\n      <td>0.480711</td>\n      <td>0.423771</td>\n      <td>0.437856</td>\n      <td>0.423713</td>\n      <td>0.429666</td>\n      <td>0.524148</td>\n      <td>0.417597</td>\n      <td>0.602860</td>\n      <td>...</td>\n      <td>0.513316</td>\n      <td>0.858221</td>\n      <td>0.452147</td>\n      <td>0.328513</td>\n      <td>0.468434</td>\n      <td>0.328571</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494963</td>\n      <td>0.336831</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.456216</td>\n      <td>0.336788</td>\n      <td>0.480655</td>\n      <td>0.426412</td>\n      <td>0.435809</td>\n      <td>0.426373</td>\n      <td>0.429689</td>\n      <td>0.529404</td>\n      <td>0.421553</td>\n      <td>0.602840</td>\n      <td>...</td>\n      <td>0.513248</td>\n      <td>0.858223</td>\n      <td>0.450158</td>\n      <td>0.328598</td>\n      <td>0.466386</td>\n      <td>0.331219</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494875</td>\n      <td>0.336869</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.450098</td>\n      <td>0.350250</td>\n      <td>0.478691</td>\n      <td>0.437156</td>\n      <td>0.431850</td>\n      <td>0.439808</td>\n      <td>0.429703</td>\n      <td>0.532210</td>\n      <td>0.429692</td>\n      <td>0.613710</td>\n      <td>...</td>\n      <td>0.515272</td>\n      <td>0.858092</td>\n      <td>0.448084</td>\n      <td>0.336658</td>\n      <td>0.464270</td>\n      <td>0.336669</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.492816</td>\n      <td>0.347583</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>723995</th>\n      <td>0.572323</td>\n      <td>0.249740</td>\n      <td>0.545817</td>\n      <td>0.331265</td>\n      <td>0.513305</td>\n      <td>0.334060</td>\n      <td>0.496944</td>\n      <td>0.439946</td>\n      <td>0.515239</td>\n      <td>0.507831</td>\n      <td>...</td>\n      <td>0.557984</td>\n      <td>0.743981</td>\n      <td>0.560053</td>\n      <td>0.241677</td>\n      <td>0.574331</td>\n      <td>0.236296</td>\n      <td>0.531556</td>\n      <td>0.249948</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723996</th>\n      <td>0.574264</td>\n      <td>0.247185</td>\n      <td>0.545855</td>\n      <td>0.331294</td>\n      <td>0.513342</td>\n      <td>0.334083</td>\n      <td>0.496923</td>\n      <td>0.439842</td>\n      <td>0.509138</td>\n      <td>0.507875</td>\n      <td>...</td>\n      <td>0.556023</td>\n      <td>0.743962</td>\n      <td>0.560064</td>\n      <td>0.239012</td>\n      <td>0.574341</td>\n      <td>0.233590</td>\n      <td>0.531623</td>\n      <td>0.249950</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723997</th>\n      <td>0.572278</td>\n      <td>0.247181</td>\n      <td>0.545858</td>\n      <td>0.331319</td>\n      <td>0.515255</td>\n      <td>0.334127</td>\n      <td>0.496880</td>\n      <td>0.442504</td>\n      <td>0.503084</td>\n      <td>0.521377</td>\n      <td>...</td>\n      <td>0.556000</td>\n      <td>0.744010</td>\n      <td>0.560003</td>\n      <td>0.239012</td>\n      <td>0.574309</td>\n      <td>0.233606</td>\n      <td>0.531602</td>\n      <td>0.249977</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723998</th>\n      <td>0.572309</td>\n      <td>0.247146</td>\n      <td>0.545872</td>\n      <td>0.333883</td>\n      <td>0.515258</td>\n      <td>0.336619</td>\n      <td>0.496959</td>\n      <td>0.442683</td>\n      <td>0.496922</td>\n      <td>0.537717</td>\n      <td>...</td>\n      <td>0.556006</td>\n      <td>0.744062</td>\n      <td>0.560016</td>\n      <td>0.238944</td>\n      <td>0.574327</td>\n      <td>0.233575</td>\n      <td>0.531600</td>\n      <td>0.249969</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723999</th>\n      <td>0.574313</td>\n      <td>0.247090</td>\n      <td>0.545845</td>\n      <td>0.333906</td>\n      <td>0.513322</td>\n      <td>0.336681</td>\n      <td>0.497050</td>\n      <td>0.442677</td>\n      <td>0.494947</td>\n      <td>0.534938</td>\n      <td>...</td>\n      <td>0.556038</td>\n      <td>0.744019</td>\n      <td>0.560069</td>\n      <td>0.236244</td>\n      <td>0.574359</td>\n      <td>0.230927</td>\n      <td>0.531583</td>\n      <td>0.249944</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>724000 rows Ã— 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "normalize(x_test_data)\n",
    "normalize(x_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data = y_train_data - 1\n",
    "y_test_data = y_test_data - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "181.0\n5751.0\n22625\n5751\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data.shape[0] / 32 / 125)\n",
    "print(x_test_data.shape[0] / 32)\n",
    "print(y_train_data.shape[0])\n",
    "print(y_test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:100\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(x_seq, y_seq, seq_size, batch_size):\n",
    "    for batch_pos in range(0, len(y_seq), batch_size):\n",
    "        x_batch = list()\n",
    "        for pos in range(batch_size):\n",
    "            # print(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size])\n",
    "            x_batch.append(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size].values)\n",
    "        \n",
    "        yield torch.tensor(x_batch), torch.flatten(torch.tensor(y_seq.iloc[batch_pos:batch_pos + batch_size].values))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch [76/100], Step [20/181], Loss: 1.1667\n",
      "Epoch [76/100], Step [30/181], Loss: 0.4607\n",
      "Epoch [76/100], Step [40/181], Loss: 0.2396\n",
      "Epoch [76/100], Step [50/181], Loss: 2.5845\n",
      "Epoch [76/100], Step [60/181], Loss: 0.6999\n",
      "Epoch [76/100], Step [70/181], Loss: 0.2126\n",
      "Epoch [76/100], Step [80/181], Loss: 0.1032\n",
      "Epoch [76/100], Step [90/181], Loss: 0.7912\n",
      "Epoch [76/100], Step [100/181], Loss: 0.4907\n",
      "Epoch [76/100], Step [110/181], Loss: 0.1853\n",
      "Epoch [76/100], Step [120/181], Loss: 0.1157\n",
      "Epoch [76/100], Step [130/181], Loss: 1.3566\n",
      "Epoch [76/100], Step [140/181], Loss: 1.2104\n",
      "Epoch [76/100], Step [150/181], Loss: 0.5798\n",
      "Epoch [76/100], Step [160/181], Loss: 0.3009\n",
      "Epoch [76/100], Step [170/181], Loss: 1.9937\n",
      "Epoch [76/100], Step [180/181], Loss: 1.1613\n",
      "Epoch [77/100], Step [10/181], Loss: 1.2155\n",
      "Epoch [77/100], Step [20/181], Loss: 1.4523\n",
      "Epoch [77/100], Step [30/181], Loss: 0.8044\n",
      "Epoch [77/100], Step [40/181], Loss: 0.2766\n",
      "Epoch [77/100], Step [50/181], Loss: 3.1182\n",
      "Epoch [77/100], Step [60/181], Loss: 0.1801\n",
      "Epoch [77/100], Step [70/181], Loss: 0.1423\n",
      "Epoch [77/100], Step [80/181], Loss: 0.0611\n",
      "Epoch [77/100], Step [90/181], Loss: 0.7943\n",
      "Epoch [77/100], Step [100/181], Loss: 0.2757\n",
      "Epoch [77/100], Step [110/181], Loss: 0.1963\n",
      "Epoch [77/100], Step [120/181], Loss: 0.0864\n",
      "Epoch [77/100], Step [130/181], Loss: 0.7857\n",
      "Epoch [77/100], Step [140/181], Loss: 0.8995\n",
      "Epoch [77/100], Step [150/181], Loss: 0.3234\n",
      "Epoch [77/100], Step [160/181], Loss: 0.2175\n",
      "Epoch [77/100], Step [170/181], Loss: 1.9339\n",
      "Epoch [77/100], Step [180/181], Loss: 1.2251\n",
      "Epoch [78/100], Step [10/181], Loss: 0.8388\n",
      "Epoch [78/100], Step [20/181], Loss: 1.6032\n",
      "Epoch [78/100], Step [30/181], Loss: 1.0208\n",
      "Epoch [78/100], Step [40/181], Loss: 0.3586\n",
      "Epoch [78/100], Step [50/181], Loss: 2.9566\n",
      "Epoch [78/100], Step [60/181], Loss: 1.7849\n",
      "Epoch [78/100], Step [70/181], Loss: 0.1771\n",
      "Epoch [78/100], Step [80/181], Loss: 0.1642\n",
      "Epoch [78/100], Step [90/181], Loss: 5.5433\n",
      "Epoch [78/100], Step [100/181], Loss: 3.4539\n",
      "Epoch [78/100], Step [110/181], Loss: 0.9667\n",
      "Epoch [78/100], Step [120/181], Loss: 0.1883\n",
      "Epoch [78/100], Step [130/181], Loss: 2.0484\n",
      "Epoch [78/100], Step [140/181], Loss: 0.6098\n",
      "Epoch [78/100], Step [150/181], Loss: 0.2668\n",
      "Epoch [78/100], Step [160/181], Loss: 0.1527\n",
      "Epoch [78/100], Step [170/181], Loss: 2.0221\n",
      "Epoch [78/100], Step [180/181], Loss: 1.1242\n",
      "Epoch [79/100], Step [10/181], Loss: 1.2367\n",
      "Epoch [79/100], Step [20/181], Loss: 2.3203\n",
      "Epoch [79/100], Step [30/181], Loss: 1.7371\n",
      "Epoch [79/100], Step [40/181], Loss: 0.7410\n",
      "Epoch [79/100], Step [50/181], Loss: 2.6007\n",
      "Epoch [79/100], Step [60/181], Loss: 1.0888\n",
      "Epoch [79/100], Step [70/181], Loss: 0.3635\n",
      "Epoch [79/100], Step [80/181], Loss: 0.1531\n",
      "Epoch [79/100], Step [90/181], Loss: 2.5550\n",
      "Epoch [79/100], Step [100/181], Loss: 1.0618\n",
      "Epoch [79/100], Step [110/181], Loss: 0.2223\n",
      "Epoch [79/100], Step [120/181], Loss: 0.0549\n",
      "Epoch [79/100], Step [130/181], Loss: 0.5408\n",
      "Epoch [79/100], Step [140/181], Loss: 0.3870\n",
      "Epoch [79/100], Step [150/181], Loss: 0.2534\n",
      "Epoch [79/100], Step [160/181], Loss: 0.1714\n",
      "Epoch [79/100], Step [170/181], Loss: 2.4630\n",
      "Epoch [79/100], Step [180/181], Loss: 1.2536\n",
      "Epoch [80/100], Step [10/181], Loss: 1.6435\n",
      "Epoch [80/100], Step [20/181], Loss: 3.8037\n",
      "Epoch [80/100], Step [30/181], Loss: 3.1108\n",
      "Epoch [80/100], Step [40/181], Loss: 1.0094\n",
      "Epoch [80/100], Step [50/181], Loss: 5.0109\n",
      "Epoch [80/100], Step [60/181], Loss: 1.4724\n",
      "Epoch [80/100], Step [70/181], Loss: 0.4159\n",
      "Epoch [80/100], Step [80/181], Loss: 0.0944\n",
      "Epoch [80/100], Step [90/181], Loss: 3.9892\n",
      "Epoch [80/100], Step [100/181], Loss: 1.8819\n",
      "Epoch [80/100], Step [110/181], Loss: 0.3972\n",
      "Epoch [80/100], Step [120/181], Loss: 0.0978\n",
      "Epoch [80/100], Step [130/181], Loss: 4.7136\n",
      "Epoch [80/100], Step [140/181], Loss: 1.7040\n",
      "Epoch [80/100], Step [150/181], Loss: 0.1828\n",
      "Epoch [80/100], Step [160/181], Loss: 0.0353\n",
      "Epoch [80/100], Step [170/181], Loss: 4.0599\n",
      "Epoch [80/100], Step [180/181], Loss: 1.2620\n",
      "Epoch [81/100], Step [10/181], Loss: 1.4934\n",
      "Epoch [81/100], Step [20/181], Loss: 3.0041\n",
      "Epoch [81/100], Step [30/181], Loss: 2.8012\n",
      "Epoch [81/100], Step [40/181], Loss: 1.3558\n",
      "Epoch [81/100], Step [50/181], Loss: 4.6883\n",
      "Epoch [81/100], Step [60/181], Loss: 2.1249\n",
      "Epoch [81/100], Step [70/181], Loss: 0.4381\n",
      "Epoch [81/100], Step [80/181], Loss: 0.2015\n",
      "Epoch [81/100], Step [90/181], Loss: 2.9830\n",
      "Epoch [81/100], Step [100/181], Loss: 1.9869\n",
      "Epoch [81/100], Step [110/181], Loss: 0.9828\n",
      "Epoch [81/100], Step [120/181], Loss: 0.4476\n",
      "Epoch [81/100], Step [130/181], Loss: 4.2049\n",
      "Epoch [81/100], Step [140/181], Loss: 2.5835\n",
      "Epoch [81/100], Step [150/181], Loss: 0.8434\n",
      "Epoch [81/100], Step [160/181], Loss: 0.2724\n",
      "Epoch [81/100], Step [170/181], Loss: 3.1899\n",
      "Epoch [81/100], Step [180/181], Loss: 2.0449\n",
      "Epoch [82/100], Step [10/181], Loss: 2.8070\n",
      "Epoch [82/100], Step [20/181], Loss: 3.0103\n",
      "Epoch [82/100], Step [30/181], Loss: 2.6915\n",
      "Epoch [82/100], Step [40/181], Loss: 1.6881\n",
      "Epoch [82/100], Step [50/181], Loss: 2.9413\n",
      "Epoch [82/100], Step [60/181], Loss: 2.1520\n",
      "Epoch [82/100], Step [70/181], Loss: 1.1462\n",
      "Epoch [82/100], Step [80/181], Loss: 0.5186\n",
      "Epoch [82/100], Step [90/181], Loss: 2.8909\n",
      "Epoch [82/100], Step [100/181], Loss: 2.1137\n",
      "Epoch [82/100], Step [110/181], Loss: 1.1916\n",
      "Epoch [82/100], Step [120/181], Loss: 0.6207\n",
      "Epoch [82/100], Step [130/181], Loss: 3.8644\n",
      "Epoch [82/100], Step [140/181], Loss: 2.8613\n",
      "Epoch [82/100], Step [150/181], Loss: 1.6287\n",
      "Epoch [82/100], Step [160/181], Loss: 0.7545\n",
      "Epoch [82/100], Step [170/181], Loss: 2.8682\n",
      "Epoch [82/100], Step [180/181], Loss: 2.1947\n",
      "Epoch [83/100], Step [10/181], Loss: 2.4073\n",
      "Epoch [83/100], Step [20/181], Loss: 2.4383\n",
      "Epoch [83/100], Step [30/181], Loss: 2.2642\n",
      "Epoch [83/100], Step [40/181], Loss: 1.6312\n",
      "Epoch [83/100], Step [50/181], Loss: 2.8725\n",
      "Epoch [83/100], Step [60/181], Loss: 2.3386\n",
      "Epoch [83/100], Step [70/181], Loss: 1.7285\n",
      "Epoch [83/100], Step [80/181], Loss: 1.1609\n",
      "Epoch [83/100], Step [90/181], Loss: 2.5422\n",
      "Epoch [83/100], Step [100/181], Loss: 2.0144\n",
      "Epoch [83/100], Step [110/181], Loss: 1.3654\n",
      "Epoch [83/100], Step [120/181], Loss: 0.8364\n",
      "Epoch [83/100], Step [130/181], Loss: 3.0889\n",
      "Epoch [83/100], Step [140/181], Loss: 2.4265\n",
      "Epoch [83/100], Step [150/181], Loss: 1.5952\n",
      "Epoch [83/100], Step [160/181], Loss: 0.9896\n",
      "Epoch [83/100], Step [170/181], Loss: 2.5172\n",
      "Epoch [83/100], Step [180/181], Loss: 1.9386\n",
      "Epoch [84/100], Step [10/181], Loss: 2.6443\n",
      "Epoch [84/100], Step [20/181], Loss: 2.7951\n",
      "Epoch [84/100], Step [30/181], Loss: 2.5118\n",
      "Epoch [84/100], Step [40/181], Loss: 1.6891\n",
      "Epoch [84/100], Step [50/181], Loss: 2.3331\n",
      "Epoch [84/100], Step [60/181], Loss: 2.0113\n",
      "Epoch [84/100], Step [70/181], Loss: 1.5038\n",
      "Epoch [84/100], Step [80/181], Loss: 1.0559\n",
      "Epoch [84/100], Step [90/181], Loss: 2.5836\n",
      "Epoch [84/100], Step [100/181], Loss: 2.1416\n",
      "Epoch [84/100], Step [110/181], Loss: 1.5546\n",
      "Epoch [84/100], Step [120/181], Loss: 1.0560\n",
      "Epoch [84/100], Step [130/181], Loss: 2.7422\n",
      "Epoch [84/100], Step [140/181], Loss: 2.1589\n",
      "Epoch [84/100], Step [150/181], Loss: 1.5178\n",
      "Epoch [84/100], Step [160/181], Loss: 1.0703\n",
      "Epoch [84/100], Step [170/181], Loss: 2.7050\n",
      "Epoch [84/100], Step [180/181], Loss: 2.1889\n",
      "Epoch [85/100], Step [10/181], Loss: 2.6357\n",
      "Epoch [85/100], Step [20/181], Loss: 2.5073\n",
      "Epoch [85/100], Step [30/181], Loss: 2.3256\n",
      "Epoch [85/100], Step [40/181], Loss: 1.8336\n",
      "Epoch [85/100], Step [50/181], Loss: 2.4666\n",
      "Epoch [85/100], Step [60/181], Loss: 2.1548\n",
      "Epoch [85/100], Step [70/181], Loss: 1.6143\n",
      "Epoch [85/100], Step [80/181], Loss: 1.1759\n",
      "Epoch [85/100], Step [90/181], Loss: 2.4207\n",
      "Epoch [85/100], Step [100/181], Loss: 2.0459\n",
      "Epoch [85/100], Step [110/181], Loss: 1.5413\n",
      "Epoch [85/100], Step [120/181], Loss: 1.1012\n",
      "Epoch [85/100], Step [130/181], Loss: 2.2960\n",
      "Epoch [85/100], Step [140/181], Loss: 1.9524\n",
      "Epoch [85/100], Step [150/181], Loss: 1.5208\n",
      "Epoch [85/100], Step [160/181], Loss: 1.1396\n",
      "Epoch [85/100], Step [170/181], Loss: 2.9551\n",
      "Epoch [85/100], Step [180/181], Loss: 2.4502\n",
      "Epoch [86/100], Step [10/181], Loss: 2.5387\n",
      "Epoch [86/100], Step [20/181], Loss: 2.3702\n",
      "Epoch [86/100], Step [30/181], Loss: 2.2137\n",
      "Epoch [86/100], Step [40/181], Loss: 1.7687\n",
      "Epoch [86/100], Step [50/181], Loss: 2.2832\n",
      "Epoch [86/100], Step [60/181], Loss: 2.0484\n",
      "Epoch [86/100], Step [70/181], Loss: 1.6349\n",
      "Epoch [86/100], Step [80/181], Loss: 1.2275\n",
      "Epoch [86/100], Step [90/181], Loss: 2.3784\n",
      "Epoch [86/100], Step [100/181], Loss: 2.0117\n",
      "Epoch [86/100], Step [110/181], Loss: 1.5166\n",
      "Epoch [86/100], Step [120/181], Loss: 1.0860\n",
      "Epoch [86/100], Step [130/181], Loss: 2.4511\n",
      "Epoch [86/100], Step [140/181], Loss: 1.9267\n",
      "Epoch [86/100], Step [150/181], Loss: 1.4323\n",
      "Epoch [86/100], Step [160/181], Loss: 1.0708\n",
      "Epoch [86/100], Step [170/181], Loss: 2.8828\n",
      "Epoch [86/100], Step [180/181], Loss: 2.4218\n",
      "Epoch [87/100], Step [10/181], Loss: 2.5407\n",
      "Epoch [87/100], Step [20/181], Loss: 2.3628\n",
      "Epoch [87/100], Step [30/181], Loss: 2.2125\n",
      "Epoch [87/100], Step [40/181], Loss: 1.7889\n",
      "Epoch [87/100], Step [50/181], Loss: 2.3199\n",
      "Epoch [87/100], Step [60/181], Loss: 2.0926\n",
      "Epoch [87/100], Step [70/181], Loss: 1.6914\n",
      "Epoch [87/100], Step [80/181], Loss: 1.2936\n",
      "Epoch [87/100], Step [90/181], Loss: 2.3091\n",
      "Epoch [87/100], Step [100/181], Loss: 2.0170\n",
      "Epoch [87/100], Step [110/181], Loss: 1.5956\n",
      "Epoch [87/100], Step [120/181], Loss: 1.2010\n",
      "Epoch [87/100], Step [130/181], Loss: 2.2080\n",
      "Epoch [87/100], Step [140/181], Loss: 1.8885\n",
      "Epoch [87/100], Step [150/181], Loss: 1.4970\n",
      "Epoch [87/100], Step [160/181], Loss: 1.1508\n",
      "Epoch [87/100], Step [170/181], Loss: 2.8198\n",
      "Epoch [87/100], Step [180/181], Loss: 2.3695\n",
      "Epoch [88/100], Step [10/181], Loss: 2.5814\n",
      "Epoch [88/100], Step [20/181], Loss: 2.3620\n",
      "Epoch [88/100], Step [30/181], Loss: 2.1957\n",
      "Epoch [88/100], Step [40/181], Loss: 1.7857\n",
      "Epoch [88/100], Step [50/181], Loss: 2.2512\n",
      "Epoch [88/100], Step [60/181], Loss: 2.0438\n",
      "Epoch [88/100], Step [70/181], Loss: 1.6692\n",
      "Epoch [88/100], Step [80/181], Loss: 1.2942\n",
      "Epoch [88/100], Step [90/181], Loss: 2.2606\n",
      "Epoch [88/100], Step [100/181], Loss: 1.9846\n",
      "Epoch [88/100], Step [110/181], Loss: 1.5856\n",
      "Epoch [88/100], Step [120/181], Loss: 1.2084\n",
      "Epoch [88/100], Step [130/181], Loss: 2.2252\n",
      "Epoch [88/100], Step [140/181], Loss: 1.8924\n",
      "Epoch [88/100], Step [150/181], Loss: 1.5003\n",
      "Epoch [88/100], Step [160/181], Loss: 1.1592\n",
      "Epoch [88/100], Step [170/181], Loss: 2.8054\n",
      "Epoch [88/100], Step [180/181], Loss: 2.3712\n",
      "Epoch [89/100], Step [10/181], Loss: 2.5693\n",
      "Epoch [89/100], Step [20/181], Loss: 2.3514\n",
      "Epoch [89/100], Step [30/181], Loss: 2.1858\n",
      "Epoch [89/100], Step [40/181], Loss: 1.7771\n",
      "Epoch [89/100], Step [50/181], Loss: 2.2363\n",
      "Epoch [89/100], Step [60/181], Loss: 2.0328\n",
      "Epoch [89/100], Step [70/181], Loss: 1.6577\n",
      "Epoch [89/100], Step [80/181], Loss: 1.2782\n",
      "Epoch [89/100], Step [90/181], Loss: 2.2160\n",
      "Epoch [89/100], Step [100/181], Loss: 1.8952\n",
      "Epoch [89/100], Step [110/181], Loss: 1.4490\n",
      "Epoch [89/100], Step [120/181], Loss: 1.0602\n",
      "Epoch [89/100], Step [130/181], Loss: 2.6042\n",
      "Epoch [89/100], Step [140/181], Loss: 2.1558\n",
      "Epoch [89/100], Step [150/181], Loss: 1.5316\n",
      "Epoch [89/100], Step [160/181], Loss: 1.1051\n",
      "Epoch [89/100], Step [170/181], Loss: 2.7392\n",
      "Epoch [89/100], Step [180/181], Loss: 2.3318\n",
      "Epoch [90/100], Step [10/181], Loss: 2.5331\n",
      "Epoch [90/100], Step [20/181], Loss: 2.3361\n",
      "Epoch [90/100], Step [30/181], Loss: 2.1844\n",
      "Epoch [90/100], Step [40/181], Loss: 1.7921\n",
      "Epoch [90/100], Step [50/181], Loss: 2.2766\n",
      "Epoch [90/100], Step [60/181], Loss: 2.0551\n",
      "Epoch [90/100], Step [70/181], Loss: 1.6842\n",
      "Epoch [90/100], Step [80/181], Loss: 1.3209\n",
      "Epoch [90/100], Step [90/181], Loss: 2.2693\n",
      "Epoch [90/100], Step [100/181], Loss: 2.0098\n",
      "Epoch [90/100], Step [110/181], Loss: 1.6293\n",
      "Epoch [90/100], Step [120/181], Loss: 1.2633\n",
      "Epoch [90/100], Step [130/181], Loss: 2.1486\n",
      "Epoch [90/100], Step [140/181], Loss: 1.8656\n",
      "Epoch [90/100], Step [150/181], Loss: 1.5027\n",
      "Epoch [90/100], Step [160/181], Loss: 1.1727\n",
      "Epoch [90/100], Step [170/181], Loss: 2.7747\n",
      "Epoch [90/100], Step [180/181], Loss: 2.3619\n",
      "Epoch [91/100], Step [10/181], Loss: 2.5565\n",
      "Epoch [91/100], Step [20/181], Loss: 2.3560\n",
      "Epoch [91/100], Step [30/181], Loss: 2.2054\n",
      "Epoch [91/100], Step [40/181], Loss: 1.8118\n",
      "Epoch [91/100], Step [50/181], Loss: 2.2051\n",
      "Epoch [91/100], Step [60/181], Loss: 2.0168\n",
      "Epoch [91/100], Step [70/181], Loss: 1.6706\n",
      "Epoch [91/100], Step [80/181], Loss: 1.3221\n",
      "Epoch [91/100], Step [90/181], Loss: 2.1859\n",
      "Epoch [91/100], Step [100/181], Loss: 1.9378\n",
      "Epoch [91/100], Step [110/181], Loss: 1.5751\n",
      "Epoch [91/100], Step [120/181], Loss: 1.2255\n",
      "Epoch [91/100], Step [130/181], Loss: 2.1766\n",
      "Epoch [91/100], Step [140/181], Loss: 1.8803\n",
      "Epoch [91/100], Step [150/181], Loss: 1.5090\n",
      "Epoch [91/100], Step [160/181], Loss: 1.1738\n",
      "Epoch [91/100], Step [170/181], Loss: 2.8202\n",
      "Epoch [91/100], Step [180/181], Loss: 2.4072\n",
      "Epoch [92/100], Step [10/181], Loss: 2.5756\n",
      "Epoch [92/100], Step [20/181], Loss: 2.3339\n",
      "Epoch [92/100], Step [30/181], Loss: 2.1785\n",
      "Epoch [92/100], Step [40/181], Loss: 1.7962\n",
      "Epoch [92/100], Step [50/181], Loss: 2.1573\n",
      "Epoch [92/100], Step [60/181], Loss: 1.9778\n",
      "Epoch [92/100], Step [70/181], Loss: 1.6396\n",
      "Epoch [92/100], Step [80/181], Loss: 1.2881\n",
      "Epoch [92/100], Step [90/181], Loss: 2.1661\n",
      "Epoch [92/100], Step [100/181], Loss: 1.8741\n",
      "Epoch [92/100], Step [110/181], Loss: 1.4450\n",
      "Epoch [92/100], Step [120/181], Loss: 1.0576\n",
      "Epoch [92/100], Step [130/181], Loss: 2.5801\n",
      "Epoch [92/100], Step [140/181], Loss: 2.1841\n",
      "Epoch [92/100], Step [150/181], Loss: 1.6118\n",
      "Epoch [92/100], Step [160/181], Loss: 1.1596\n",
      "Epoch [92/100], Step [170/181], Loss: 2.7257\n",
      "Epoch [92/100], Step [180/181], Loss: 2.3134\n",
      "Epoch [93/100], Step [10/181], Loss: 2.4977\n",
      "Epoch [93/100], Step [20/181], Loss: 2.2954\n",
      "Epoch [93/100], Step [30/181], Loss: 2.1508\n",
      "Epoch [93/100], Step [40/181], Loss: 1.7704\n",
      "Epoch [93/100], Step [50/181], Loss: 2.2111\n",
      "Epoch [93/100], Step [60/181], Loss: 2.0200\n",
      "Epoch [93/100], Step [70/181], Loss: 1.6729\n",
      "Epoch [93/100], Step [80/181], Loss: 1.3232\n",
      "Epoch [93/100], Step [90/181], Loss: 2.2613\n",
      "Epoch [93/100], Step [100/181], Loss: 2.0139\n",
      "Epoch [93/100], Step [110/181], Loss: 1.6478\n",
      "Epoch [93/100], Step [120/181], Loss: 1.2933\n",
      "Epoch [93/100], Step [130/181], Loss: 2.1325\n",
      "Epoch [93/100], Step [140/181], Loss: 1.8680\n",
      "Epoch [93/100], Step [150/181], Loss: 1.5207\n",
      "Epoch [93/100], Step [160/181], Loss: 1.1960\n",
      "Epoch [93/100], Step [170/181], Loss: 2.7917\n",
      "Epoch [93/100], Step [180/181], Loss: 2.3825\n",
      "Epoch [94/100], Step [10/181], Loss: 2.5281\n",
      "Epoch [94/100], Step [20/181], Loss: 2.3324\n",
      "Epoch [94/100], Step [30/181], Loss: 2.2087\n",
      "Epoch [94/100], Step [40/181], Loss: 1.8447\n",
      "Epoch [94/100], Step [50/181], Loss: 2.1640\n",
      "Epoch [94/100], Step [60/181], Loss: 1.9946\n",
      "Epoch [94/100], Step [70/181], Loss: 1.6767\n",
      "Epoch [94/100], Step [80/181], Loss: 1.3489\n",
      "Epoch [94/100], Step [90/181], Loss: 2.1638\n",
      "Epoch [94/100], Step [100/181], Loss: 1.9340\n",
      "Epoch [94/100], Step [110/181], Loss: 1.5945\n",
      "Epoch [94/100], Step [120/181], Loss: 1.2653\n",
      "Epoch [94/100], Step [130/181], Loss: 2.1248\n",
      "Epoch [94/100], Step [140/181], Loss: 1.8722\n",
      "Epoch [94/100], Step [150/181], Loss: 1.5380\n",
      "Epoch [94/100], Step [160/181], Loss: 1.2223\n",
      "Epoch [94/100], Step [170/181], Loss: 2.7697\n",
      "Epoch [94/100], Step [180/181], Loss: 2.3819\n",
      "Epoch [95/100], Step [10/181], Loss: 2.5265\n",
      "Epoch [95/100], Step [20/181], Loss: 2.3143\n",
      "Epoch [95/100], Step [30/181], Loss: 2.1917\n",
      "Epoch [95/100], Step [40/181], Loss: 1.8377\n",
      "Epoch [95/100], Step [50/181], Loss: 2.1278\n",
      "Epoch [95/100], Step [60/181], Loss: 1.9648\n",
      "Epoch [95/100], Step [70/181], Loss: 1.6565\n",
      "Epoch [95/100], Step [80/181], Loss: 1.3382\n",
      "Epoch [95/100], Step [90/181], Loss: 2.1492\n",
      "Epoch [95/100], Step [100/181], Loss: 1.9247\n",
      "Epoch [95/100], Step [110/181], Loss: 1.5916\n",
      "Epoch [95/100], Step [120/181], Loss: 1.2677\n",
      "Epoch [95/100], Step [130/181], Loss: 2.1247\n",
      "Epoch [95/100], Step [140/181], Loss: 1.8770\n",
      "Epoch [95/100], Step [150/181], Loss: 1.5496\n",
      "Epoch [95/100], Step [160/181], Loss: 1.2391\n",
      "Epoch [95/100], Step [170/181], Loss: 2.7642\n",
      "Epoch [95/100], Step [180/181], Loss: 2.3834\n",
      "Epoch [96/100], Step [10/181], Loss: 2.5111\n",
      "Epoch [96/100], Step [20/181], Loss: 2.3082\n",
      "Epoch [96/100], Step [30/181], Loss: 2.1874\n",
      "Epoch [96/100], Step [40/181], Loss: 1.8413\n",
      "Epoch [96/100], Step [50/181], Loss: 2.1144\n",
      "Epoch [96/100], Step [60/181], Loss: 1.9549\n",
      "Epoch [96/100], Step [70/181], Loss: 1.6548\n",
      "Epoch [96/100], Step [80/181], Loss: 1.3437\n",
      "Epoch [96/100], Step [90/181], Loss: 2.1329\n",
      "Epoch [96/100], Step [100/181], Loss: 1.9171\n",
      "Epoch [96/100], Step [110/181], Loss: 1.5932\n",
      "Epoch [96/100], Step [120/181], Loss: 1.2748\n",
      "Epoch [96/100], Step [130/181], Loss: 2.1066\n",
      "Epoch [96/100], Step [140/181], Loss: 1.8635\n",
      "Epoch [96/100], Step [150/181], Loss: 1.5429\n",
      "Epoch [96/100], Step [160/181], Loss: 1.2398\n",
      "Epoch [96/100], Step [170/181], Loss: 2.7609\n",
      "Epoch [96/100], Step [180/181], Loss: 2.3892\n",
      "Epoch [97/100], Step [10/181], Loss: 2.5177\n",
      "Epoch [97/100], Step [20/181], Loss: 2.3015\n",
      "Epoch [97/100], Step [30/181], Loss: 2.1766\n",
      "Epoch [97/100], Step [40/181], Loss: 1.8332\n",
      "Epoch [97/100], Step [50/181], Loss: 2.1090\n",
      "Epoch [97/100], Step [60/181], Loss: 1.9512\n",
      "Epoch [97/100], Step [70/181], Loss: 1.6536\n",
      "Epoch [97/100], Step [80/181], Loss: 1.3447\n",
      "Epoch [97/100], Step [90/181], Loss: 2.1324\n",
      "Epoch [97/100], Step [100/181], Loss: 1.9196\n",
      "Epoch [97/100], Step [110/181], Loss: 1.5991\n",
      "Epoch [97/100], Step [120/181], Loss: 1.2833\n",
      "Epoch [97/100], Step [130/181], Loss: 2.0919\n",
      "Epoch [97/100], Step [140/181], Loss: 1.8536\n",
      "Epoch [97/100], Step [150/181], Loss: 1.5384\n",
      "Epoch [97/100], Step [160/181], Loss: 1.2393\n",
      "Epoch [97/100], Step [170/181], Loss: 2.7642\n",
      "Epoch [97/100], Step [180/181], Loss: 2.3954\n",
      "Epoch [98/100], Step [10/181], Loss: 2.5223\n",
      "Epoch [98/100], Step [20/181], Loss: 2.2982\n",
      "Epoch [98/100], Step [30/181], Loss: 2.1722\n",
      "Epoch [98/100], Step [40/181], Loss: 1.8307\n",
      "Epoch [98/100], Step [50/181], Loss: 2.1035\n",
      "Epoch [98/100], Step [60/181], Loss: 1.9472\n",
      "Epoch [98/100], Step [70/181], Loss: 1.6516\n",
      "Epoch [98/100], Step [80/181], Loss: 1.3445\n",
      "Epoch [98/100], Step [90/181], Loss: 2.1296\n",
      "Epoch [98/100], Step [100/181], Loss: 1.9194\n",
      "Epoch [98/100], Step [110/181], Loss: 1.6016\n",
      "Epoch [98/100], Step [120/181], Loss: 1.2881\n",
      "Epoch [98/100], Step [130/181], Loss: 2.0849\n",
      "Epoch [98/100], Step [140/181], Loss: 1.8494\n",
      "Epoch [98/100], Step [150/181], Loss: 1.5375\n",
      "Epoch [98/100], Step [160/181], Loss: 1.2407\n",
      "Epoch [98/100], Step [170/181], Loss: 2.7650\n",
      "Epoch [98/100], Step [180/181], Loss: 2.3984\n",
      "Epoch [99/100], Step [10/181], Loss: 2.5238\n",
      "Epoch [99/100], Step [20/181], Loss: 2.2963\n",
      "Epoch [99/100], Step [30/181], Loss: 2.1703\n",
      "Epoch [99/100], Step [40/181], Loss: 1.8304\n",
      "Epoch [99/100], Step [50/181], Loss: 2.0990\n",
      "Epoch [99/100], Step [60/181], Loss: 1.9438\n",
      "Epoch [99/100], Step [70/181], Loss: 1.6498\n",
      "Epoch [99/100], Step [80/181], Loss: 1.3442\n",
      "Epoch [99/100], Step [90/181], Loss: 2.1267\n",
      "Epoch [99/100], Step [100/181], Loss: 1.9185\n",
      "Epoch [99/100], Step [110/181], Loss: 1.6030\n",
      "Epoch [99/100], Step [120/181], Loss: 1.2913\n",
      "Epoch [99/100], Step [130/181], Loss: 2.0807\n",
      "Epoch [99/100], Step [140/181], Loss: 1.8473\n",
      "Epoch [99/100], Step [150/181], Loss: 1.5376\n",
      "Epoch [99/100], Step [160/181], Loss: 1.2423\n",
      "Epoch [99/100], Step [170/181], Loss: 2.7649\n",
      "Epoch [99/100], Step [180/181], Loss: 2.3999\n",
      "Epoch [100/100], Step [10/181], Loss: 2.5240\n",
      "Epoch [100/100], Step [20/181], Loss: 2.2950\n",
      "Epoch [100/100], Step [30/181], Loss: 2.1694\n",
      "Epoch [100/100], Step [40/181], Loss: 1.8308\n",
      "Epoch [100/100], Step [50/181], Loss: 2.0955\n",
      "Epoch [100/100], Step [60/181], Loss: 1.9410\n",
      "Epoch [100/100], Step [70/181], Loss: 1.6483\n",
      "Epoch [100/100], Step [80/181], Loss: 1.3440\n",
      "Epoch [100/100], Step [90/181], Loss: 2.1241\n",
      "Epoch [100/100], Step [100/181], Loss: 1.9176\n",
      "Epoch [100/100], Step [110/181], Loss: 1.6040\n",
      "Epoch [100/100], Step [120/181], Loss: 1.2939\n",
      "Epoch [100/100], Step [130/181], Loss: 2.0778\n",
      "Epoch [100/100], Step [140/181], Loss: 1.8459\n",
      "Epoch [100/100], Step [150/181], Loss: 1.5378\n",
      "Epoch [100/100], Step [160/181], Loss: 1.2437\n",
      "Epoch [100/100], Step [170/181], Loss: 2.7643\n",
      "Epoch [100/100], Step [180/181], Loss: 2.4007\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_total_steps = int(y_train_data.shape[0] / train_batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x_batch, y_batch) in enumerate(chunker(x_train_data, y_train_data, sequence_length, train_batch_size)):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        y_batch = y_batch.to(device)\n",
    "        x_batch = x_batch.to(device)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "        if (i+1) % 10 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 10000 test images: 22.830812032689966 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(chunker(x_test_data, y_test_data, sequence_length, test_batch_size)):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        y_batch = y_batch.to(device)\n",
    "        x_batch = x_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        \n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        n_samples += y_batch.size(0)\n",
    "        n_correct += (predicted == y_batch).sum().item()\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}