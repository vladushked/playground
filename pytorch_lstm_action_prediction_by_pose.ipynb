{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit29e064361c954e90adf267cf1683fa25",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "DATASET_PATH = \"poses_dataset/\"\n",
    "\n",
    "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 7\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 36\n",
    "sequence_length = 32\n",
    "hidden_size = 128\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [  \"j0_x\",  \"j0_y\", \"j1_x\", \"j1_y\" , \"j2_x\", \"j2_y\", \"j3_x\", \"j3_y\", \"j4_x\", \"j4_y\", \"j5_x\", \"j5_y\", \"j6_x\", \"j6_y\", \"j7_x\", \"j7_y\", \"j8_x\", \"j8_y\", \"j9_x\", \"j9_y\", \"j10_x\", \"j10_y\", \"j11_x\", \"j11_y\", \"j12_x\", \"j12_y\", \"j13_x\", \"j13_y\", 'j14_x', \"j14_y\", \"j15_x\", \"j15_y\", \"j16_x\", \"j16_y\", \"j17_x\", \"j17_y\" ]\n",
    "x_train_data = pd.read_csv(X_train_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "x_test_data = pd.read_csv(X_test_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "y_train_data = pd.read_csv(y_train_path, names=[\"labels\"], dtype=np.int_)\n",
    "y_test_data = pd.read_csv(y_test_path, names=[\"labels\"], dtype=np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    for i, column in enumerate(data):\n",
    "        if i % 2 == 0:\n",
    "            data[column] = data[column] / 640\n",
    "        else:\n",
    "            data[column] = data[column] / 480\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            j0_x      j0_y      j1_x      j1_y      j2_x      j2_y      j3_x  \\\n",
       "0       0.462366  0.336623  0.480770  0.423777  0.439916  0.423683  0.429683   \n",
       "1       0.462273  0.336667  0.480756  0.423767  0.439889  0.423719  0.429670   \n",
       "2       0.458275  0.336708  0.480711  0.423771  0.437856  0.423713  0.429666   \n",
       "3       0.456216  0.336788  0.480655  0.426412  0.435809  0.426373  0.429689   \n",
       "4       0.450098  0.350250  0.478691  0.437156  0.431850  0.439808  0.429703   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "723995  0.572323  0.249740  0.545817  0.331265  0.513305  0.334060  0.496944   \n",
       "723996  0.574264  0.247185  0.545855  0.331294  0.513342  0.334083  0.496923   \n",
       "723997  0.572278  0.247181  0.545858  0.331319  0.515255  0.334127  0.496880   \n",
       "723998  0.572309  0.247146  0.545872  0.333883  0.515258  0.336619  0.496959   \n",
       "723999  0.574313  0.247090  0.545845  0.333906  0.513322  0.336681  0.497050   \n",
       "\n",
       "            j3_y      j4_x      j4_y  ...     j13_x     j13_y     j14_x  \\\n",
       "0       0.524087  0.417491  0.610944  ...  0.513325  0.858171  0.460228   \n",
       "1       0.524113  0.417486  0.608252  ...  0.513270  0.855479  0.458206   \n",
       "2       0.524148  0.417597  0.602860  ...  0.513316  0.858221  0.452147   \n",
       "3       0.529404  0.421553  0.602840  ...  0.513248  0.858223  0.450158   \n",
       "4       0.532210  0.429692  0.613710  ...  0.515272  0.858092  0.448084   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "723995  0.439946  0.515239  0.507831  ...  0.557984  0.743981  0.560053   \n",
       "723996  0.439842  0.509138  0.507875  ...  0.556023  0.743962  0.560064   \n",
       "723997  0.442504  0.503084  0.521377  ...  0.556000  0.744010  0.560003   \n",
       "723998  0.442683  0.496922  0.537717  ...  0.556006  0.744062  0.560016   \n",
       "723999  0.442677  0.494947  0.534938  ...  0.556038  0.744019  0.560069   \n",
       "\n",
       "           j14_y     j15_x     j15_y     j16_x     j16_y     j17_x     j17_y  \n",
       "0       0.325875  0.476566  0.325871  0.000000  0.000000  0.497005  0.336733  \n",
       "1       0.328513  0.474541  0.328554  0.000000  0.000000  0.496912  0.336779  \n",
       "2       0.328513  0.468434  0.328571  0.000000  0.000000  0.494963  0.336831  \n",
       "3       0.328598  0.466386  0.331219  0.000000  0.000000  0.494875  0.336869  \n",
       "4       0.336658  0.464270  0.336669  0.000000  0.000000  0.492816  0.347583  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "723995  0.241677  0.574331  0.236296  0.531556  0.249948  0.000000  0.000000  \n",
       "723996  0.239012  0.574341  0.233590  0.531623  0.249950  0.000000  0.000000  \n",
       "723997  0.239012  0.574309  0.233606  0.531602  0.249977  0.000000  0.000000  \n",
       "723998  0.238944  0.574327  0.233575  0.531600  0.249969  0.000000  0.000000  \n",
       "723999  0.236244  0.574359  0.230927  0.531583  0.249944  0.000000  0.000000  \n",
       "\n",
       "[724000 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j0_x</th>\n      <th>j0_y</th>\n      <th>j1_x</th>\n      <th>j1_y</th>\n      <th>j2_x</th>\n      <th>j2_y</th>\n      <th>j3_x</th>\n      <th>j3_y</th>\n      <th>j4_x</th>\n      <th>j4_y</th>\n      <th>...</th>\n      <th>j13_x</th>\n      <th>j13_y</th>\n      <th>j14_x</th>\n      <th>j14_y</th>\n      <th>j15_x</th>\n      <th>j15_y</th>\n      <th>j16_x</th>\n      <th>j16_y</th>\n      <th>j17_x</th>\n      <th>j17_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.462366</td>\n      <td>0.336623</td>\n      <td>0.480770</td>\n      <td>0.423777</td>\n      <td>0.439916</td>\n      <td>0.423683</td>\n      <td>0.429683</td>\n      <td>0.524087</td>\n      <td>0.417491</td>\n      <td>0.610944</td>\n      <td>...</td>\n      <td>0.513325</td>\n      <td>0.858171</td>\n      <td>0.460228</td>\n      <td>0.325875</td>\n      <td>0.476566</td>\n      <td>0.325871</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.497005</td>\n      <td>0.336733</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.462273</td>\n      <td>0.336667</td>\n      <td>0.480756</td>\n      <td>0.423767</td>\n      <td>0.439889</td>\n      <td>0.423719</td>\n      <td>0.429670</td>\n      <td>0.524113</td>\n      <td>0.417486</td>\n      <td>0.608252</td>\n      <td>...</td>\n      <td>0.513270</td>\n      <td>0.855479</td>\n      <td>0.458206</td>\n      <td>0.328513</td>\n      <td>0.474541</td>\n      <td>0.328554</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.496912</td>\n      <td>0.336779</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.458275</td>\n      <td>0.336708</td>\n      <td>0.480711</td>\n      <td>0.423771</td>\n      <td>0.437856</td>\n      <td>0.423713</td>\n      <td>0.429666</td>\n      <td>0.524148</td>\n      <td>0.417597</td>\n      <td>0.602860</td>\n      <td>...</td>\n      <td>0.513316</td>\n      <td>0.858221</td>\n      <td>0.452147</td>\n      <td>0.328513</td>\n      <td>0.468434</td>\n      <td>0.328571</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494963</td>\n      <td>0.336831</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.456216</td>\n      <td>0.336788</td>\n      <td>0.480655</td>\n      <td>0.426412</td>\n      <td>0.435809</td>\n      <td>0.426373</td>\n      <td>0.429689</td>\n      <td>0.529404</td>\n      <td>0.421553</td>\n      <td>0.602840</td>\n      <td>...</td>\n      <td>0.513248</td>\n      <td>0.858223</td>\n      <td>0.450158</td>\n      <td>0.328598</td>\n      <td>0.466386</td>\n      <td>0.331219</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494875</td>\n      <td>0.336869</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.450098</td>\n      <td>0.350250</td>\n      <td>0.478691</td>\n      <td>0.437156</td>\n      <td>0.431850</td>\n      <td>0.439808</td>\n      <td>0.429703</td>\n      <td>0.532210</td>\n      <td>0.429692</td>\n      <td>0.613710</td>\n      <td>...</td>\n      <td>0.515272</td>\n      <td>0.858092</td>\n      <td>0.448084</td>\n      <td>0.336658</td>\n      <td>0.464270</td>\n      <td>0.336669</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.492816</td>\n      <td>0.347583</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>723995</th>\n      <td>0.572323</td>\n      <td>0.249740</td>\n      <td>0.545817</td>\n      <td>0.331265</td>\n      <td>0.513305</td>\n      <td>0.334060</td>\n      <td>0.496944</td>\n      <td>0.439946</td>\n      <td>0.515239</td>\n      <td>0.507831</td>\n      <td>...</td>\n      <td>0.557984</td>\n      <td>0.743981</td>\n      <td>0.560053</td>\n      <td>0.241677</td>\n      <td>0.574331</td>\n      <td>0.236296</td>\n      <td>0.531556</td>\n      <td>0.249948</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723996</th>\n      <td>0.574264</td>\n      <td>0.247185</td>\n      <td>0.545855</td>\n      <td>0.331294</td>\n      <td>0.513342</td>\n      <td>0.334083</td>\n      <td>0.496923</td>\n      <td>0.439842</td>\n      <td>0.509138</td>\n      <td>0.507875</td>\n      <td>...</td>\n      <td>0.556023</td>\n      <td>0.743962</td>\n      <td>0.560064</td>\n      <td>0.239012</td>\n      <td>0.574341</td>\n      <td>0.233590</td>\n      <td>0.531623</td>\n      <td>0.249950</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723997</th>\n      <td>0.572278</td>\n      <td>0.247181</td>\n      <td>0.545858</td>\n      <td>0.331319</td>\n      <td>0.515255</td>\n      <td>0.334127</td>\n      <td>0.496880</td>\n      <td>0.442504</td>\n      <td>0.503084</td>\n      <td>0.521377</td>\n      <td>...</td>\n      <td>0.556000</td>\n      <td>0.744010</td>\n      <td>0.560003</td>\n      <td>0.239012</td>\n      <td>0.574309</td>\n      <td>0.233606</td>\n      <td>0.531602</td>\n      <td>0.249977</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723998</th>\n      <td>0.572309</td>\n      <td>0.247146</td>\n      <td>0.545872</td>\n      <td>0.333883</td>\n      <td>0.515258</td>\n      <td>0.336619</td>\n      <td>0.496959</td>\n      <td>0.442683</td>\n      <td>0.496922</td>\n      <td>0.537717</td>\n      <td>...</td>\n      <td>0.556006</td>\n      <td>0.744062</td>\n      <td>0.560016</td>\n      <td>0.238944</td>\n      <td>0.574327</td>\n      <td>0.233575</td>\n      <td>0.531600</td>\n      <td>0.249969</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723999</th>\n      <td>0.574313</td>\n      <td>0.247090</td>\n      <td>0.545845</td>\n      <td>0.333906</td>\n      <td>0.513322</td>\n      <td>0.336681</td>\n      <td>0.497050</td>\n      <td>0.442677</td>\n      <td>0.494947</td>\n      <td>0.534938</td>\n      <td>...</td>\n      <td>0.556038</td>\n      <td>0.744019</td>\n      <td>0.560069</td>\n      <td>0.236244</td>\n      <td>0.574359</td>\n      <td>0.230927</td>\n      <td>0.531583</td>\n      <td>0.249944</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>724000 rows × 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "normalize(x_test_data)\n",
    "normalize(x_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             j0_x      j0_y      j1_x      j1_y      j2_x      j2_y      j3_x  \\\n",
       "0       0.480608  0.339533  0.499006  0.429050  0.458230  0.426417  0.445991   \n",
       "1       0.480573  0.339540  0.499003  0.429056  0.458214  0.426448  0.445981   \n",
       "2       0.478591  0.339481  0.498986  0.429035  0.456236  0.426415  0.445988   \n",
       "3       0.478508  0.336883  0.498959  0.426423  0.456230  0.426333  0.445983   \n",
       "4       0.476562  0.336775  0.497027  0.426348  0.454209  0.423727  0.445997   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "184027  0.572295  0.247225  0.545781  0.331338  0.513191  0.334079  0.494963   \n",
       "184028  0.572328  0.247229  0.545794  0.333888  0.513186  0.336617  0.494970   \n",
       "184029  0.572286  0.247194  0.545811  0.333858  0.513228  0.334133  0.494969   \n",
       "184030  0.572294  0.247144  0.545798  0.333860  0.513242  0.336677  0.494942   \n",
       "184031  0.572277  0.247154  0.545809  0.331348  0.513237  0.336660  0.494948   \n",
       "\n",
       "            j3_y      j4_x      j4_y  ...     j13_x     j13_y     j14_x  \\\n",
       "0       0.521419  0.433775  0.597352  ...  0.527416  0.858235  0.476553   \n",
       "1       0.521417  0.433775  0.594800  ...  0.527414  0.858202  0.476514   \n",
       "2       0.524065  0.433777  0.597444  ...  0.527422  0.858233  0.472530   \n",
       "3       0.521427  0.431887  0.597506  ...  0.527445  0.858206  0.470505   \n",
       "4       0.521408  0.431880  0.597565  ...  0.527423  0.858217  0.470452   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "184027  0.442633  0.501048  0.532175  ...  0.556022  0.744021  0.560039   \n",
       "184028  0.442600  0.496945  0.532337  ...  0.556008  0.744029  0.560070   \n",
       "184029  0.439887  0.494936  0.526819  ...  0.556017  0.744033  0.560014   \n",
       "184030  0.442646  0.494873  0.529490  ...  0.556022  0.744015  0.559997   \n",
       "184031  0.442579  0.494861  0.529612  ...  0.556014  0.744002  0.559992   \n",
       "\n",
       "           j14_y     j15_x     j15_y     j16_x     j16_y     j17_x     j17_y  \n",
       "0       0.333940  0.492905  0.333894  0.000000  0.000000  0.515238  0.336773  \n",
       "1       0.333948  0.492866  0.333892  0.000000  0.000000  0.513323  0.336781  \n",
       "2       0.333890  0.490823  0.333875  0.000000  0.000000  0.513273  0.336835  \n",
       "3       0.331300  0.488778  0.331308  0.000000  0.000000  0.513252  0.336785  \n",
       "4       0.328621  0.486744  0.331202  0.000000  0.000000  0.513173  0.336777  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "184027  0.239006  0.574270  0.236288  0.531545  0.249973  0.000000  0.000000  \n",
       "184028  0.241612  0.574289  0.236290  0.531577  0.249985  0.000000  0.000000  \n",
       "184029  0.238992  0.574297  0.236213  0.531559  0.249929  0.000000  0.000000  \n",
       "184030  0.238963  0.574316  0.233525  0.531567  0.249938  0.000000  0.000000  \n",
       "184031  0.238967  0.574317  0.233540  0.531575  0.249925  0.000000  0.000000  \n",
       "\n",
       "[184032 rows x 36 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "x_test_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "type(y_train_data.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(x_seq, y_seq, seq_size, batch_size):\n",
    "    for batch_pos in range(0, len(y_seq), batch_size):\n",
    "        x_batch = list()\n",
    "        for pos in range(batch_size):\n",
    "            # print(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size])\n",
    "            x_batch.append(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size].values)\n",
    "        \n",
    "        yield torch.tensor(x_batch), torch.flatten(torch.tensor(y_seq.iloc[batch_pos:batch_pos + batch_size].values))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1000, 32, 36])\n",
      "torch.Size([1000, 32, 36])\n",
      "torch.Size([1000, 32, 36])\n",
      "torch.Size([1000, 32, 36])\n",
      "torch.Size([1000, 32, 36])\n",
      "torch.Size([1000, 32, 36])\n"
     ]
    }
   ],
   "source": [
    "for i, (x_batch, y_batch) in enumerate(chunker(x_test_data, y_test_data, sequence_length, batch_size)):\n",
    "    if i == 100:\n",
    "        break\n",
    "    print(x_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/100], Step [1/707.03125], Loss: 2.4927\n",
      "Epoch [1/100], Step [2/707.03125], Loss: 2.4772\n",
      "Epoch [1/100], Step [3/707.03125], Loss: 2.4448\n",
      "Epoch [1/100], Step [4/707.03125], Loss: 2.4001\n",
      "Epoch [1/100], Step [5/707.03125], Loss: 2.3467\n",
      "Epoch [1/100], Step [6/707.03125], Loss: 2.2876\n",
      "Epoch [1/100], Step [7/707.03125], Loss: 2.2249\n",
      "Epoch [1/100], Step [8/707.03125], Loss: 2.1599\n",
      "Epoch [1/100], Step [9/707.03125], Loss: 2.0937\n",
      "Epoch [1/100], Step [10/707.03125], Loss: 2.0270\n",
      "Epoch [1/100], Step [11/707.03125], Loss: 1.9599\n",
      "Epoch [1/100], Step [12/707.03125], Loss: 1.8928\n",
      "Epoch [1/100], Step [13/707.03125], Loss: 1.8254\n",
      "Epoch [1/100], Step [14/707.03125], Loss: 1.7577\n",
      "Epoch [1/100], Step [15/707.03125], Loss: 1.6895\n",
      "Epoch [1/100], Step [16/707.03125], Loss: 1.6204\n",
      "Epoch [1/100], Step [17/707.03125], Loss: 1.5503\n",
      "Epoch [1/100], Step [18/707.03125], Loss: 1.4792\n",
      "Epoch [1/100], Step [19/707.03125], Loss: 1.4070\n",
      "Epoch [1/100], Step [20/707.03125], Loss: 1.3343\n",
      "Epoch [1/100], Step [21/707.03125], Loss: 1.2616\n",
      "Epoch [1/100], Step [22/707.03125], Loss: 1.1898\n",
      "Epoch [1/100], Step [23/707.03125], Loss: 1.1196\n",
      "Epoch [1/100], Step [24/707.03125], Loss: 1.0513\n",
      "Epoch [1/100], Step [25/707.03125], Loss: 2.3862\n",
      "Epoch [1/100], Step [26/707.03125], Loss: 2.3961\n",
      "Epoch [1/100], Step [27/707.03125], Loss: 2.3903\n",
      "Epoch [1/100], Step [28/707.03125], Loss: 2.3711\n",
      "Epoch [1/100], Step [29/707.03125], Loss: 2.3402\n",
      "Epoch [1/100], Step [30/707.03125], Loss: 2.2996\n",
      "Epoch [1/100], Step [31/707.03125], Loss: 2.2509\n",
      "Epoch [1/100], Step [32/707.03125], Loss: 2.1956\n",
      "Epoch [1/100], Step [33/707.03125], Loss: 2.1351\n",
      "Epoch [1/100], Step [34/707.03125], Loss: 2.0705\n",
      "Epoch [1/100], Step [35/707.03125], Loss: 2.0029\n",
      "Epoch [1/100], Step [36/707.03125], Loss: 1.9331\n",
      "Epoch [1/100], Step [37/707.03125], Loss: 1.8620\n",
      "Epoch [1/100], Step [38/707.03125], Loss: 1.7902\n",
      "Epoch [1/100], Step [39/707.03125], Loss: 1.7182\n",
      "Epoch [1/100], Step [40/707.03125], Loss: 1.6464\n",
      "Epoch [1/100], Step [41/707.03125], Loss: 1.5750\n",
      "Epoch [1/100], Step [42/707.03125], Loss: 1.5041\n",
      "Epoch [1/100], Step [43/707.03125], Loss: 1.4339\n",
      "Epoch [1/100], Step [44/707.03125], Loss: 1.3643\n",
      "Epoch [1/100], Step [45/707.03125], Loss: 1.2954\n",
      "Epoch [1/100], Step [46/707.03125], Loss: 1.2273\n",
      "Epoch [1/100], Step [47/707.03125], Loss: 1.1602\n",
      "Epoch [1/100], Step [48/707.03125], Loss: 1.0939\n",
      "Epoch [1/100], Step [49/707.03125], Loss: 1.0288\n",
      "Epoch [1/100], Step [50/707.03125], Loss: 0.9648\n",
      "Epoch [1/100], Step [51/707.03125], Loss: 0.9022\n",
      "Epoch [1/100], Step [52/707.03125], Loss: 0.8410\n",
      "Epoch [1/100], Step [53/707.03125], Loss: 0.7816\n",
      "Epoch [1/100], Step [54/707.03125], Loss: 0.7240\n",
      "Epoch [1/100], Step [55/707.03125], Loss: 0.6687\n",
      "Epoch [1/100], Step [56/707.03125], Loss: 0.6157\n",
      "Epoch [1/100], Step [57/707.03125], Loss: 0.5653\n",
      "Epoch [1/100], Step [58/707.03125], Loss: 0.5177\n",
      "Epoch [1/100], Step [59/707.03125], Loss: 0.4731\n",
      "Epoch [1/100], Step [60/707.03125], Loss: 1.7551\n",
      "Epoch [1/100], Step [61/707.03125], Loss: 2.8769\n",
      "Epoch [1/100], Step [62/707.03125], Loss: 2.8813\n",
      "Epoch [1/100], Step [63/707.03125], Loss: 2.8569\n",
      "Epoch [1/100], Step [64/707.03125], Loss: 2.8087\n",
      "Epoch [1/100], Step [65/707.03125], Loss: 2.7412\n",
      "Epoch [1/100], Step [66/707.03125], Loss: 2.6586\n",
      "Epoch [1/100], Step [67/707.03125], Loss: 2.5645\n",
      "Epoch [1/100], Step [68/707.03125], Loss: 2.4620\n",
      "Epoch [1/100], Step [69/707.03125], Loss: 2.3539\n",
      "Epoch [1/100], Step [70/707.03125], Loss: 2.2428\n",
      "Epoch [1/100], Step [71/707.03125], Loss: 2.1309\n",
      "Epoch [1/100], Step [72/707.03125], Loss: 2.0198\n",
      "Epoch [1/100], Step [73/707.03125], Loss: 1.9106\n",
      "Epoch [1/100], Step [74/707.03125], Loss: 1.8038\n",
      "Epoch [1/100], Step [75/707.03125], Loss: 1.6997\n",
      "Epoch [1/100], Step [76/707.03125], Loss: 1.5986\n",
      "Epoch [1/100], Step [77/707.03125], Loss: 1.5006\n",
      "Epoch [1/100], Step [78/707.03125], Loss: 1.4055\n",
      "Epoch [1/100], Step [79/707.03125], Loss: 1.3134\n",
      "Epoch [1/100], Step [80/707.03125], Loss: 1.2245\n",
      "Epoch [1/100], Step [81/707.03125], Loss: 1.1389\n",
      "Epoch [1/100], Step [82/707.03125], Loss: 1.0567\n",
      "Epoch [1/100], Step [83/707.03125], Loss: 0.9781\n",
      "Epoch [1/100], Step [84/707.03125], Loss: 0.9032\n",
      "Epoch [1/100], Step [85/707.03125], Loss: 0.8321\n",
      "Epoch [1/100], Step [86/707.03125], Loss: 0.7649\n",
      "Epoch [1/100], Step [87/707.03125], Loss: 0.7019\n",
      "Epoch [1/100], Step [88/707.03125], Loss: 0.6431\n",
      "Epoch [1/100], Step [89/707.03125], Loss: 0.5884\n",
      "Epoch [1/100], Step [90/707.03125], Loss: 0.5379\n",
      "Epoch [1/100], Step [91/707.03125], Loss: 0.4915\n",
      "Epoch [1/100], Step [92/707.03125], Loss: 0.4488\n",
      "Epoch [1/100], Step [93/707.03125], Loss: 0.4098\n",
      "Epoch [1/100], Step [94/707.03125], Loss: 0.3742\n",
      "Epoch [1/100], Step [95/707.03125], Loss: 0.3418\n",
      "Epoch [1/100], Step [96/707.03125], Loss: 0.3123\n",
      "Epoch [1/100], Step [97/707.03125], Loss: 0.2855\n",
      "Epoch [1/100], Step [98/707.03125], Loss: 0.2613\n",
      "Epoch [1/100], Step [99/707.03125], Loss: 0.2394\n",
      "Epoch [1/100], Step [100/707.03125], Loss: 0.2196\n",
      "Epoch [1/100], Step [101/707.03125], Loss: 0.2017\n",
      "Epoch [1/100], Step [102/707.03125], Loss: 0.1855\n",
      "Epoch [1/100], Step [103/707.03125], Loss: 0.1709\n",
      "Epoch [1/100], Step [104/707.03125], Loss: 0.1578\n",
      "Epoch [1/100], Step [105/707.03125], Loss: 0.1459\n",
      "Epoch [1/100], Step [106/707.03125], Loss: 0.1353\n",
      "Epoch [1/100], Step [107/707.03125], Loss: 1.3471\n",
      "Epoch [1/100], Step [108/707.03125], Loss: 4.0964\n",
      "Epoch [1/100], Step [109/707.03125], Loss: 4.0638\n",
      "Epoch [1/100], Step [110/707.03125], Loss: 3.9850\n",
      "Epoch [1/100], Step [111/707.03125], Loss: 3.8701\n",
      "Epoch [1/100], Step [112/707.03125], Loss: 3.7272\n",
      "Epoch [1/100], Step [113/707.03125], Loss: 3.5622\n",
      "Epoch [1/100], Step [114/707.03125], Loss: 3.3797\n",
      "Epoch [1/100], Step [115/707.03125], Loss: 3.1831\n",
      "Epoch [1/100], Step [116/707.03125], Loss: 2.9755\n",
      "Epoch [1/100], Step [117/707.03125], Loss: 2.7605\n",
      "Epoch [1/100], Step [118/707.03125], Loss: 2.5420\n",
      "Epoch [1/100], Step [119/707.03125], Loss: 2.3240\n",
      "Epoch [1/100], Step [120/707.03125], Loss: 2.1090\n",
      "Epoch [1/100], Step [121/707.03125], Loss: 1.8984\n",
      "Epoch [1/100], Step [122/707.03125], Loss: 1.6938\n",
      "Epoch [1/100], Step [123/707.03125], Loss: 1.4978\n",
      "Epoch [1/100], Step [124/707.03125], Loss: 1.3162\n",
      "Epoch [1/100], Step [125/707.03125], Loss: 1.1540\n",
      "Epoch [1/100], Step [126/707.03125], Loss: 1.0127\n",
      "Epoch [1/100], Step [127/707.03125], Loss: 0.8907\n",
      "Epoch [1/100], Step [128/707.03125], Loss: 0.7855\n",
      "Epoch [1/100], Step [129/707.03125], Loss: 0.6948\n",
      "Epoch [1/100], Step [130/707.03125], Loss: 0.6164\n",
      "Epoch [1/100], Step [131/707.03125], Loss: 0.5481\n",
      "Epoch [1/100], Step [132/707.03125], Loss: 0.4884\n",
      "Epoch [1/100], Step [133/707.03125], Loss: 0.4361\n",
      "Epoch [1/100], Step [134/707.03125], Loss: 0.3904\n",
      "Epoch [1/100], Step [135/707.03125], Loss: 0.3503\n",
      "Epoch [1/100], Step [136/707.03125], Loss: 0.3152\n",
      "Epoch [1/100], Step [137/707.03125], Loss: 0.2845\n",
      "Epoch [1/100], Step [138/707.03125], Loss: 0.2576\n",
      "Epoch [1/100], Step [139/707.03125], Loss: 0.2339\n",
      "Epoch [1/100], Step [140/707.03125], Loss: 0.2132\n",
      "Epoch [1/100], Step [141/707.03125], Loss: 0.1949\n",
      "Epoch [1/100], Step [142/707.03125], Loss: 0.1788\n",
      "Epoch [1/100], Step [143/707.03125], Loss: 0.1646\n",
      "Epoch [1/100], Step [144/707.03125], Loss: 0.1520\n",
      "Epoch [1/100], Step [145/707.03125], Loss: 0.1408\n",
      "Epoch [1/100], Step [146/707.03125], Loss: 0.1309\n",
      "Epoch [1/100], Step [147/707.03125], Loss: 0.1221\n",
      "Epoch [1/100], Step [148/707.03125], Loss: 0.1142\n",
      "Epoch [1/100], Step [149/707.03125], Loss: 0.1071\n",
      "Epoch [1/100], Step [150/707.03125], Loss: 0.1007\n",
      "Epoch [1/100], Step [151/707.03125], Loss: 0.0950\n",
      "Epoch [1/100], Step [152/707.03125], Loss: 0.0898\n",
      "Epoch [1/100], Step [153/707.03125], Loss: 0.0851\n",
      "Epoch [1/100], Step [154/707.03125], Loss: 0.0809\n",
      "Epoch [1/100], Step [155/707.03125], Loss: 3.2085\n",
      "Epoch [1/100], Step [156/707.03125], Loss: 4.4682\n",
      "Epoch [1/100], Step [157/707.03125], Loss: 4.3822\n",
      "Epoch [1/100], Step [158/707.03125], Loss: 4.2449\n",
      "Epoch [1/100], Step [159/707.03125], Loss: 4.0620\n",
      "Epoch [1/100], Step [160/707.03125], Loss: 3.8305\n",
      "Epoch [1/100], Step [161/707.03125], Loss: 3.5542\n",
      "Epoch [1/100], Step [162/707.03125], Loss: 3.2618\n",
      "Epoch [1/100], Step [163/707.03125], Loss: 2.9697\n",
      "Epoch [1/100], Step [164/707.03125], Loss: 2.6527\n",
      "Epoch [1/100], Step [165/707.03125], Loss: 2.2732\n",
      "Epoch [1/100], Step [166/707.03125], Loss: 1.8718\n",
      "Epoch [1/100], Step [167/707.03125], Loss: 1.5504\n",
      "Epoch [1/100], Step [168/707.03125], Loss: 1.3334\n",
      "Epoch [1/100], Step [169/707.03125], Loss: 1.1889\n",
      "Epoch [1/100], Step [170/707.03125], Loss: 1.0844\n",
      "Epoch [1/100], Step [171/707.03125], Loss: 0.9980\n",
      "Epoch [1/100], Step [172/707.03125], Loss: 0.9157\n",
      "Epoch [1/100], Step [173/707.03125], Loss: 0.8304\n",
      "Epoch [1/100], Step [174/707.03125], Loss: 0.7403\n",
      "Epoch [1/100], Step [175/707.03125], Loss: 0.6487\n",
      "Epoch [1/100], Step [176/707.03125], Loss: 0.5611\n",
      "Epoch [1/100], Step [177/707.03125], Loss: 0.4818\n",
      "Epoch [1/100], Step [178/707.03125], Loss: 0.4121\n",
      "Epoch [1/100], Step [179/707.03125], Loss: 0.3518\n",
      "Epoch [1/100], Step [180/707.03125], Loss: 0.2997\n",
      "Epoch [1/100], Step [181/707.03125], Loss: 0.2550\n",
      "Epoch [1/100], Step [182/707.03125], Loss: 0.2169\n",
      "Epoch [1/100], Step [183/707.03125], Loss: 0.1844\n",
      "Epoch [1/100], Step [184/707.03125], Loss: 0.1570\n",
      "Epoch [1/100], Step [185/707.03125], Loss: 0.1340\n",
      "Epoch [1/100], Step [186/707.03125], Loss: 0.1146\n",
      "Epoch [1/100], Step [187/707.03125], Loss: 0.0984\n",
      "Epoch [1/100], Step [188/707.03125], Loss: 0.0849\n",
      "Epoch [1/100], Step [189/707.03125], Loss: 0.0737\n",
      "Epoch [1/100], Step [190/707.03125], Loss: 0.0643\n",
      "Epoch [1/100], Step [191/707.03125], Loss: 0.0564\n",
      "Epoch [1/100], Step [192/707.03125], Loss: 0.0498\n",
      "Epoch [1/100], Step [193/707.03125], Loss: 0.0443\n",
      "Epoch [1/100], Step [194/707.03125], Loss: 0.0396\n",
      "Epoch [1/100], Step [195/707.03125], Loss: 0.0356\n",
      "Epoch [1/100], Step [196/707.03125], Loss: 0.0322\n",
      "Epoch [1/100], Step [197/707.03125], Loss: 0.0293\n",
      "Epoch [1/100], Step [198/707.03125], Loss: 0.0268\n",
      "Epoch [1/100], Step [199/707.03125], Loss: 0.0246\n",
      "Epoch [1/100], Step [200/707.03125], Loss: 0.0227\n",
      "Epoch [1/100], Step [201/707.03125], Loss: 0.0211\n",
      "Epoch [1/100], Step [202/707.03125], Loss: 0.0196\n",
      "Epoch [1/100], Step [203/707.03125], Loss: 0.0183\n",
      "Epoch [1/100], Step [204/707.03125], Loss: 0.0172\n",
      "Epoch [1/100], Step [205/707.03125], Loss: 0.9188\n",
      "Epoch [1/100], Step [206/707.03125], Loss: 5.6655\n",
      "Epoch [1/100], Step [207/707.03125], Loss: 5.4930\n",
      "Epoch [1/100], Step [208/707.03125], Loss: 5.2129\n",
      "Epoch [1/100], Step [209/707.03125], Loss: 4.8614\n",
      "Epoch [1/100], Step [210/707.03125], Loss: 4.4620\n",
      "Epoch [1/100], Step [211/707.03125], Loss: 4.0299\n",
      "Epoch [1/100], Step [212/707.03125], Loss: 3.5741\n",
      "Epoch [1/100], Step [213/707.03125], Loss: 3.0991\n",
      "Epoch [1/100], Step [214/707.03125], Loss: 2.6059\n",
      "Epoch [1/100], Step [215/707.03125], Loss: 2.1036\n",
      "Epoch [1/100], Step [216/707.03125], Loss: 1.6192\n",
      "Epoch [1/100], Step [217/707.03125], Loss: 1.1783\n",
      "Epoch [1/100], Step [218/707.03125], Loss: 0.7310\n",
      "Epoch [1/100], Step [219/707.03125], Loss: 0.3841\n",
      "Epoch [1/100], Step [220/707.03125], Loss: 0.2794\n",
      "Epoch [1/100], Step [221/707.03125], Loss: 0.1737\n",
      "Epoch [1/100], Step [222/707.03125], Loss: 0.1125\n",
      "Epoch [1/100], Step [223/707.03125], Loss: 0.0778\n",
      "Epoch [1/100], Step [224/707.03125], Loss: 0.0563\n",
      "Epoch [1/100], Step [225/707.03125], Loss: 0.0422\n",
      "Epoch [1/100], Step [226/707.03125], Loss: 0.0325\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (100) to match target batch_size (25).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-98a188b571bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (100) to match target batch_size (25)."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_total_steps = y_train_data.shape[0] / 32\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x_batch, y_batch) in enumerate(chunker(x_train_data, y_train_data, sequence_length, batch_size)):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        y_batch = y_batch.to(device)\n",
    "        x_batch = x_batch.to(device)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "        # if (i+1) % 100 == 0:\n",
    "        #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            # break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 10000 test images: 94.92 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}