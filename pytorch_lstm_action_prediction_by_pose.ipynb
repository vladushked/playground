{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit29e064361c954e90adf267cf1683fa25",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "LABELS = [    \n",
    "    \"JUMPING\",\n",
    "    \"JUMPING_JACKS\",\n",
    "    \"BOXING\",\n",
    "    \"WAVING_2HANDS\",\n",
    "    \"WAVING_1HAND\",\n",
    "    \"CLAPPING_HANDS\"\n",
    "\n",
    "] \n",
    "DATASET_PATH = \"poses_dataset/\"\n",
    "\n",
    "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "# Hyper-parameters \n",
    "# input_size = 784 # 28x28\n",
    "num_classes = 7\n",
    "num_epochs = 2\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 36\n",
    "sequence_length = 32\n",
    "hidden_size = 128\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [  \"j0_x\",  \"j0_y\", \"j1_x\", \"j1_y\" , \"j2_x\", \"j2_y\", \"j3_x\", \"j3_y\", \"j4_x\", \"j4_y\", \"j5_x\", \"j5_y\", \"j6_x\", \"j6_y\", \"j7_x\", \"j7_y\", \"j8_x\", \"j8_y\", \"j9_x\", \"j9_y\", \"j10_x\", \"j10_y\", \"j11_x\", \"j11_y\", \"j12_x\", \"j12_y\", \"j13_x\", \"j13_y\", 'j14_x', \"j14_y\", \"j15_x\", \"j15_y\", \"j16_x\", \"j16_y\", \"j17_x\", \"j17_y\" ]\n",
    "x_train_data = pd.read_csv(X_train_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "x_test_data = pd.read_csv(X_test_path, sep=\",\", names=column_names, header=None, dtype=np.float32)\n",
    "y_train_data = pd.read_csv(y_train_path, names=[\"labels\"], dtype=np.int_)\n",
    "y_test_data = pd.read_csv(y_test_path, names=[\"labels\"], dtype=np.int_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    for i, column in enumerate(data):\n",
    "        if i % 2 == 0:\n",
    "            data[column] = data[column] / 640\n",
    "        else:\n",
    "            data[column] = data[column] / 480\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            j0_x      j0_y      j1_x      j1_y      j2_x      j2_y      j3_x  \\\n",
       "0       0.462366  0.336623  0.480770  0.423777  0.439916  0.423683  0.429683   \n",
       "1       0.462273  0.336667  0.480756  0.423767  0.439889  0.423719  0.429670   \n",
       "2       0.458275  0.336708  0.480711  0.423771  0.437856  0.423713  0.429666   \n",
       "3       0.456216  0.336788  0.480655  0.426412  0.435809  0.426373  0.429689   \n",
       "4       0.450098  0.350250  0.478691  0.437156  0.431850  0.439808  0.429703   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "723995  0.572323  0.249740  0.545817  0.331265  0.513305  0.334060  0.496944   \n",
       "723996  0.574264  0.247185  0.545855  0.331294  0.513342  0.334083  0.496923   \n",
       "723997  0.572278  0.247181  0.545858  0.331319  0.515255  0.334127  0.496880   \n",
       "723998  0.572309  0.247146  0.545872  0.333883  0.515258  0.336619  0.496959   \n",
       "723999  0.574313  0.247090  0.545845  0.333906  0.513322  0.336681  0.497050   \n",
       "\n",
       "            j3_y      j4_x      j4_y  ...     j13_x     j13_y     j14_x  \\\n",
       "0       0.524087  0.417491  0.610944  ...  0.513325  0.858171  0.460228   \n",
       "1       0.524113  0.417486  0.608252  ...  0.513270  0.855479  0.458206   \n",
       "2       0.524148  0.417597  0.602860  ...  0.513316  0.858221  0.452147   \n",
       "3       0.529404  0.421553  0.602840  ...  0.513248  0.858223  0.450158   \n",
       "4       0.532210  0.429692  0.613710  ...  0.515272  0.858092  0.448084   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "723995  0.439946  0.515239  0.507831  ...  0.557984  0.743981  0.560053   \n",
       "723996  0.439842  0.509138  0.507875  ...  0.556023  0.743962  0.560064   \n",
       "723997  0.442504  0.503084  0.521377  ...  0.556000  0.744010  0.560003   \n",
       "723998  0.442683  0.496922  0.537717  ...  0.556006  0.744062  0.560016   \n",
       "723999  0.442677  0.494947  0.534938  ...  0.556038  0.744019  0.560069   \n",
       "\n",
       "           j14_y     j15_x     j15_y     j16_x     j16_y     j17_x     j17_y  \n",
       "0       0.325875  0.476566  0.325871  0.000000  0.000000  0.497005  0.336733  \n",
       "1       0.328513  0.474541  0.328554  0.000000  0.000000  0.496912  0.336779  \n",
       "2       0.328513  0.468434  0.328571  0.000000  0.000000  0.494963  0.336831  \n",
       "3       0.328598  0.466386  0.331219  0.000000  0.000000  0.494875  0.336869  \n",
       "4       0.336658  0.464270  0.336669  0.000000  0.000000  0.492816  0.347583  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "723995  0.241677  0.574331  0.236296  0.531556  0.249948  0.000000  0.000000  \n",
       "723996  0.239012  0.574341  0.233590  0.531623  0.249950  0.000000  0.000000  \n",
       "723997  0.239012  0.574309  0.233606  0.531602  0.249977  0.000000  0.000000  \n",
       "723998  0.238944  0.574327  0.233575  0.531600  0.249969  0.000000  0.000000  \n",
       "723999  0.236244  0.574359  0.230927  0.531583  0.249944  0.000000  0.000000  \n",
       "\n",
       "[724000 rows x 36 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>j0_x</th>\n      <th>j0_y</th>\n      <th>j1_x</th>\n      <th>j1_y</th>\n      <th>j2_x</th>\n      <th>j2_y</th>\n      <th>j3_x</th>\n      <th>j3_y</th>\n      <th>j4_x</th>\n      <th>j4_y</th>\n      <th>...</th>\n      <th>j13_x</th>\n      <th>j13_y</th>\n      <th>j14_x</th>\n      <th>j14_y</th>\n      <th>j15_x</th>\n      <th>j15_y</th>\n      <th>j16_x</th>\n      <th>j16_y</th>\n      <th>j17_x</th>\n      <th>j17_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.462366</td>\n      <td>0.336623</td>\n      <td>0.480770</td>\n      <td>0.423777</td>\n      <td>0.439916</td>\n      <td>0.423683</td>\n      <td>0.429683</td>\n      <td>0.524087</td>\n      <td>0.417491</td>\n      <td>0.610944</td>\n      <td>...</td>\n      <td>0.513325</td>\n      <td>0.858171</td>\n      <td>0.460228</td>\n      <td>0.325875</td>\n      <td>0.476566</td>\n      <td>0.325871</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.497005</td>\n      <td>0.336733</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.462273</td>\n      <td>0.336667</td>\n      <td>0.480756</td>\n      <td>0.423767</td>\n      <td>0.439889</td>\n      <td>0.423719</td>\n      <td>0.429670</td>\n      <td>0.524113</td>\n      <td>0.417486</td>\n      <td>0.608252</td>\n      <td>...</td>\n      <td>0.513270</td>\n      <td>0.855479</td>\n      <td>0.458206</td>\n      <td>0.328513</td>\n      <td>0.474541</td>\n      <td>0.328554</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.496912</td>\n      <td>0.336779</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.458275</td>\n      <td>0.336708</td>\n      <td>0.480711</td>\n      <td>0.423771</td>\n      <td>0.437856</td>\n      <td>0.423713</td>\n      <td>0.429666</td>\n      <td>0.524148</td>\n      <td>0.417597</td>\n      <td>0.602860</td>\n      <td>...</td>\n      <td>0.513316</td>\n      <td>0.858221</td>\n      <td>0.452147</td>\n      <td>0.328513</td>\n      <td>0.468434</td>\n      <td>0.328571</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494963</td>\n      <td>0.336831</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.456216</td>\n      <td>0.336788</td>\n      <td>0.480655</td>\n      <td>0.426412</td>\n      <td>0.435809</td>\n      <td>0.426373</td>\n      <td>0.429689</td>\n      <td>0.529404</td>\n      <td>0.421553</td>\n      <td>0.602840</td>\n      <td>...</td>\n      <td>0.513248</td>\n      <td>0.858223</td>\n      <td>0.450158</td>\n      <td>0.328598</td>\n      <td>0.466386</td>\n      <td>0.331219</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494875</td>\n      <td>0.336869</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.450098</td>\n      <td>0.350250</td>\n      <td>0.478691</td>\n      <td>0.437156</td>\n      <td>0.431850</td>\n      <td>0.439808</td>\n      <td>0.429703</td>\n      <td>0.532210</td>\n      <td>0.429692</td>\n      <td>0.613710</td>\n      <td>...</td>\n      <td>0.515272</td>\n      <td>0.858092</td>\n      <td>0.448084</td>\n      <td>0.336658</td>\n      <td>0.464270</td>\n      <td>0.336669</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.492816</td>\n      <td>0.347583</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>723995</th>\n      <td>0.572323</td>\n      <td>0.249740</td>\n      <td>0.545817</td>\n      <td>0.331265</td>\n      <td>0.513305</td>\n      <td>0.334060</td>\n      <td>0.496944</td>\n      <td>0.439946</td>\n      <td>0.515239</td>\n      <td>0.507831</td>\n      <td>...</td>\n      <td>0.557984</td>\n      <td>0.743981</td>\n      <td>0.560053</td>\n      <td>0.241677</td>\n      <td>0.574331</td>\n      <td>0.236296</td>\n      <td>0.531556</td>\n      <td>0.249948</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723996</th>\n      <td>0.574264</td>\n      <td>0.247185</td>\n      <td>0.545855</td>\n      <td>0.331294</td>\n      <td>0.513342</td>\n      <td>0.334083</td>\n      <td>0.496923</td>\n      <td>0.439842</td>\n      <td>0.509138</td>\n      <td>0.507875</td>\n      <td>...</td>\n      <td>0.556023</td>\n      <td>0.743962</td>\n      <td>0.560064</td>\n      <td>0.239012</td>\n      <td>0.574341</td>\n      <td>0.233590</td>\n      <td>0.531623</td>\n      <td>0.249950</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723997</th>\n      <td>0.572278</td>\n      <td>0.247181</td>\n      <td>0.545858</td>\n      <td>0.331319</td>\n      <td>0.515255</td>\n      <td>0.334127</td>\n      <td>0.496880</td>\n      <td>0.442504</td>\n      <td>0.503084</td>\n      <td>0.521377</td>\n      <td>...</td>\n      <td>0.556000</td>\n      <td>0.744010</td>\n      <td>0.560003</td>\n      <td>0.239012</td>\n      <td>0.574309</td>\n      <td>0.233606</td>\n      <td>0.531602</td>\n      <td>0.249977</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723998</th>\n      <td>0.572309</td>\n      <td>0.247146</td>\n      <td>0.545872</td>\n      <td>0.333883</td>\n      <td>0.515258</td>\n      <td>0.336619</td>\n      <td>0.496959</td>\n      <td>0.442683</td>\n      <td>0.496922</td>\n      <td>0.537717</td>\n      <td>...</td>\n      <td>0.556006</td>\n      <td>0.744062</td>\n      <td>0.560016</td>\n      <td>0.238944</td>\n      <td>0.574327</td>\n      <td>0.233575</td>\n      <td>0.531600</td>\n      <td>0.249969</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>723999</th>\n      <td>0.574313</td>\n      <td>0.247090</td>\n      <td>0.545845</td>\n      <td>0.333906</td>\n      <td>0.513322</td>\n      <td>0.336681</td>\n      <td>0.497050</td>\n      <td>0.442677</td>\n      <td>0.494947</td>\n      <td>0.534938</td>\n      <td>...</td>\n      <td>0.556038</td>\n      <td>0.744019</td>\n      <td>0.560069</td>\n      <td>0.236244</td>\n      <td>0.574359</td>\n      <td>0.230927</td>\n      <td>0.531583</td>\n      <td>0.249944</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>724000 rows Ã— 36 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "normalize(x_test_data)\n",
    "normalize(x_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of             j0_x      j0_y      j1_x      j1_y      j2_x      j2_y      j3_x  \\\n",
       "0       0.480608  0.339533  0.499006  0.429050  0.458230  0.426417  0.445991   \n",
       "1       0.480573  0.339540  0.499003  0.429056  0.458214  0.426448  0.445981   \n",
       "2       0.478591  0.339481  0.498986  0.429035  0.456236  0.426415  0.445988   \n",
       "3       0.478508  0.336883  0.498959  0.426423  0.456230  0.426333  0.445983   \n",
       "4       0.476562  0.336775  0.497027  0.426348  0.454209  0.423727  0.445997   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "184027  0.572295  0.247225  0.545781  0.331338  0.513191  0.334079  0.494963   \n",
       "184028  0.572328  0.247229  0.545794  0.333888  0.513186  0.336617  0.494970   \n",
       "184029  0.572286  0.247194  0.545811  0.333858  0.513228  0.334133  0.494969   \n",
       "184030  0.572294  0.247144  0.545798  0.333860  0.513242  0.336677  0.494942   \n",
       "184031  0.572277  0.247154  0.545809  0.331348  0.513237  0.336660  0.494948   \n",
       "\n",
       "            j3_y      j4_x      j4_y  ...     j13_x     j13_y     j14_x  \\\n",
       "0       0.521419  0.433775  0.597352  ...  0.527416  0.858235  0.476553   \n",
       "1       0.521417  0.433775  0.594800  ...  0.527414  0.858202  0.476514   \n",
       "2       0.524065  0.433777  0.597444  ...  0.527422  0.858233  0.472530   \n",
       "3       0.521427  0.431887  0.597506  ...  0.527445  0.858206  0.470505   \n",
       "4       0.521408  0.431880  0.597565  ...  0.527423  0.858217  0.470452   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "184027  0.442633  0.501048  0.532175  ...  0.556022  0.744021  0.560039   \n",
       "184028  0.442600  0.496945  0.532337  ...  0.556008  0.744029  0.560070   \n",
       "184029  0.439887  0.494936  0.526819  ...  0.556017  0.744033  0.560014   \n",
       "184030  0.442646  0.494873  0.529490  ...  0.556022  0.744015  0.559997   \n",
       "184031  0.442579  0.494861  0.529612  ...  0.556014  0.744002  0.559992   \n",
       "\n",
       "           j14_y     j15_x     j15_y     j16_x     j16_y     j17_x     j17_y  \n",
       "0       0.333940  0.492905  0.333894  0.000000  0.000000  0.515238  0.336773  \n",
       "1       0.333948  0.492866  0.333892  0.000000  0.000000  0.513323  0.336781  \n",
       "2       0.333890  0.490823  0.333875  0.000000  0.000000  0.513273  0.336835  \n",
       "3       0.331300  0.488778  0.331308  0.000000  0.000000  0.513252  0.336785  \n",
       "4       0.328621  0.486744  0.331202  0.000000  0.000000  0.513173  0.336777  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "184027  0.239006  0.574270  0.236288  0.531545  0.249973  0.000000  0.000000  \n",
       "184028  0.241612  0.574289  0.236290  0.531577  0.249985  0.000000  0.000000  \n",
       "184029  0.238992  0.574297  0.236213  0.531559  0.249929  0.000000  0.000000  \n",
       "184030  0.238963  0.574316  0.233525  0.531567  0.249938  0.000000  0.000000  \n",
       "184031  0.238967  0.574317  0.233540  0.531575  0.249925  0.000000  0.000000  \n",
       "\n",
       "[184032 rows x 36 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "x_test_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "type(y_train_data.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(x_seq, y_seq, seq_size, batch_size):\n",
    "    for batch_pos in range(0, len(y_seq), batch_size):\n",
    "        x_batch = list()\n",
    "        for pos in range(batch_size):\n",
    "            # print(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size])\n",
    "            x_batch.append(x_seq.iloc[pos*seq_size:pos*seq_size + seq_size].values)\n",
    "        \n",
    "        yield torch.tensor(x_batch), torch.flatten(torch.tensor(y_seq.iloc[batch_pos:batch_pos + batch_size].values))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([100, 32, 36])\n"
     ]
    }
   ],
   "source": [
    "for i, (x_batch, y_batch) in enumerate(chunker(x_test_data, y_test_data, sequence_length, batch_size)):\n",
    "    if i == 1:\n",
    "        break\n",
    "    print(x_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/2], Step [1/22625], Loss: 3.6021\n",
      "Epoch [1/2], Step [2/22625], Loss: 3.2459\n",
      "Epoch [1/2], Step [3/22625], Loss: 2.9666\n",
      "Epoch [1/2], Step [4/22625], Loss: 2.5039\n",
      "Epoch [1/2], Step [5/22625], Loss: 0.5582\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1000) to match target batch_size (751).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-719938a13ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1000) to match target batch_size (751)."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "n_total_steps = y_train_data.shape[0]\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x_batch, y_batch) in enumerate(chunker(x_test_data, y_test_data, sequence_length, batch_size)):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        y_batch = y_batch.to(device)\n",
    "        x_batch = x_batch.to(device)\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "        # if (i+1) % 100 == 0:\n",
    "        #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "            # break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of the network on the 10000 test images: 94.92 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}